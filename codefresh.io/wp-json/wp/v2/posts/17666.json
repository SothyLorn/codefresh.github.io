{"id":17666,"date":"2020-10-15T03:23:00","date_gmt":"2020-10-15T11:23:00","guid":{"rendered":"https:\/\/codefresh.io\/?p=17666"},"modified":"2021-04-19T03:01:23","modified_gmt":"2021-04-19T11:01:23","slug":"applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks","status":"publish","type":"post","link":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/","title":{"rendered":"Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS)"},"content":{"rendered":"<p>There are many articles and videos about practicing Continuous Delivery (CD) with applications, but not nearly as many for infrastructure. The same can be said for GitOps applied to infrastructure. That is a bit strange given that applications and infrastructure are almost the same today. Both are defined as code, and everyone stores code in Git repositories. Hence, GitOps is just as good of a fit for infrastructure as for anything else. Since, today, infrastructure is defined as code, there is no reason not to use &#8220;good&#8221; coding practices. So, making pull requests, validating suggested changes, and applying continuous delivery processes makes just as much sense for infrastructure as for applications.<\/p>\n<p>With that in mind, we&#8217;ll explore how to combine Continuous Delivery (CD) with GitOps and apply the processes on infrastructure. We&#8217;ll use Terraform as a way to define and apply infrastructure resources and <a href=\"https:\/\/codefresh.io\/codefresh-signup\/?utm_source=Blog&amp;utm_medium=Post&amp;utm_campaign=vfarcic-terraform-aks\">codefresh.io<\/a> to run CD pipelines. We&#8217;ll use Azure as our playground. Specifically, we&#8217;ll create, modify, and destroy an Azure Kubernetes Service (AKS) cluster.<\/p>\n<p><em>If you prefer a different cloud provider, please visit the <a href=\"https:\/\/codefresh.io\/continuous-deployment\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-aws-elastic-kubernetes-service-eks\/\">AWS<\/a> or <a href=\"https:\/\/codefresh.io\/continuous-deployment\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-google-kubernetes-engine-gke\/\">Google Cloud<\/a> versions of the article.<\/em><\/p>\n<p><em>If you are new to Codefresh, please <a href=\"https:\/\/codefresh.io\/codefresh-signup\/?utm_source=Blog&amp;utm_medium=Post&amp;utm_campaign=vfarcic-terraform-aks\">Create A Free Account<\/a>. You&#8217;ll get unlimited builds for life for free.<\/em><\/p>\n<p>That wasn&#8217;t much of an intro. I am probably supposed to explain the benefits of GitOps and Continuous Delivery. I should probably say at least a few words why I chose Terraform, Codefresh, and Azure. But I will not do that, at least not here. I will assume that you have at least a high-level understanding of those concepts, processes, and tools. Instead, we&#8217;ll dive straight into practical examples on setting up a Continuous Delivery (CD) process backed by GitOps, and focused on managing your infrastructure.<\/p>\n<p><em>If you&#8217;re interested in GitOps, I encourage you to watch <a href=\"https:\/\/youtu.be\/qwyRJlmG5ew\">What Is GitOps And Why Do We Want It?<\/a> on YouTube. As for continuous delivery, I already wrote a lot on that subject, but I&#8217;m too lazy to dig through past posts in <a href=\"https:\/\/technologyconversations.com\/\">TechnologyConversations.com<\/a>, so I&#8217;ll leave you to search through it. Similarly, I will not dive into the reasons why I chose Terraform. I already did that in <a href=\"https:\/\/www.devopstoolkitseries.com\/posts\/catalog\/\">The DevOps Toolkit: Catalog, Patterns, And Blueprints<\/a>. Get that course or purchase the book. It&#8217;ll be just enough to a Red Bull for yet another sleepless night, during which I&#8217;ll continue adding more material to it. I&#8217;ll be using <a href=\"https:\/\/codefresh.io\/codefresh-signup\/?utm_source=Blog&amp;utm_medium=Post&amp;utm_campaign=vfarcic-terraform-aks\">codefresh.io<\/a> simply because it&#8217;s awesome. Finally, I tend to use AWS, Azure, and Google Cloud equally, so I&#8217;ll probably cover the other two later. This is focused on Azure.<\/em><\/p>\n<p>How about that? That was probably the shortest introduction into something that aims at combining two processes and, at least, three different tools. I should probably get an award for skipping the theory and jumping straight into the how-to part.<\/p>\n<p>Let&#8217;s see whether we can set up and validate everything in 30 minutes or less.<\/p>\n<p><em>All the commands are available in the <a href=\"https:\/\/gist.github.com\/c5c927c6ac19f0bff37f87b3679cbf78\">01-03-terraform-aks-cf.sh<\/a> Gist. Feel free to use it if you&#8217;re too lazy to type. There&#8217;s no shame in copy &amp; paste.<\/em><\/p>\n<h2>Getting The Code<\/h2>\n<p>I already created a Git repository with Terraform and Codefresh pipeline definitions to skip the tedious part of writing <code>.tf<\/code> and <code>.yaml<\/code> files. All we have to do as a start is open the repo in a browser.<\/p>\n<p><em>If you are a Windows user, I will assume that you are running the commands from a Bourne Again Shell (Bash) or a Z Shell (Zsh) and not PowerShell. That should not be a problem if you followed the instructions on setting up Windows Subsystem for Linux (WSL) explained in the <a href=\"https:\/\/youtu.be\/zSqugKeOa1Y\">Installing Windows Subsystem For Linux (WSL)<\/a> YouTube video. If you do not like WSL, a Bash emulator like GitBash should do. If none of those is an acceptable option, you might need to modify some of the commands in the examples that follow.<\/em><\/p>\n<pre><code class=\"bash\">open https:\/\/github.com\/vfarcic\/cf-terraform-aks\n<\/code><\/pre>\n<p><em>If the <code>open<\/code> command does not work on your operating system, replace it with <code>echo<\/code>, copy the output, and paste it in your favorite browser. In this particular case, it might be easier to just copy the address, but, as you will see later, some of the other examples will generate the addresses dynamically. So, get used to <code>echo<\/code>, at least when going through my examples.<\/em><\/p>\n<p>Next, you will need to fork the repo. We&#8217;ll soon make some changes to the code, and you wouldn&#8217;t be able to push them to my repo. So, it needs to be yours.<\/p>\n<p><em>If you do not know how to fork a GitHub repo, the only thing I can say is &#8220;shame on you&#8221;. Google how to do that. I will not spend time explaining that.<\/em><\/p>\n<p>Next, we&#8217;ll clone the newly forked repository.<\/p>\n<p><em>Please replace <code>[...]<\/code> with your GitHub organization in the command that follows. If you forked the repo into your personal account, then the organization is your GitHub username.<\/em><\/p>\n<pre><code class=\"bash\"># Replace `[...]` with the GitHub organization\nexport GH_ORG=[...]\n\ngit clone https:\/\/github.com\/$GH_ORG\/cf-terraform-aks\n\ncd cf-terraform-aks\n<\/code><\/pre>\n<p>Next, we need to copy the Terraform files and Codefresh pipeline from the <code>orig<\/code> directory into the repository root. That might sound strange, but there is a good reason behind it. I might be experimenting with that repo. The files in the root might be configured with my info. To avoid any potential issues, I stored the &#8220;golden&#8221; version of the files inside that directory.<\/p>\n<pre><code class=\"bash\">cp orig\/*.tf .\n\ncp orig\/codefresh.yml .\n<\/code><\/pre>\n<p>Now that we have all the definitions, we should switch our focus towards setting up Azure pre-requisites.<\/p>\n<h2>Setting Up A Azure Resource Group<\/h2>\n<p>Before we proceed, we need to prepare our Azure account. To be more specific, we&#8217;ll create a resource group and a service principal that will allow Terraform to authenticate in Azure.<\/p>\n<p>As you hopefully know, almost everything in Azure is organized inside resource groups. So, we&#8217;ll need to create one. To do that, first, we need to ensure that you are logged in. We&#8217;ll need <code>az<\/code> CLI for that. If you do not already have it, go to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/install-azure-cli?view=azure-cli-latest\">https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/install-azure-cli?view=azure-cli-latest<\/a> section of the documentation and follow the instructions for your operating system.<\/p>\n<p>While we are in the &#8220;installing&#8221; mood, please set up <a href=\"https:\/\/stedolan.github.io\/jq\/download\/\">jq<\/a> as well. We&#8217;ll use it to parse JSON outputs.<\/p>\n<p><em>Remember that if you are using Windows Subsystem For Linux (WSL), you should follow the Linux instructions for installing CLIs.<\/em><\/p>\n<p>The first thing we&#8217;ll do is authenticate.<\/p>\n<pre><code class=\"bash\">az login\n<\/code><\/pre>\n<p>Next, we&#8217;ll create a resource group called <code>devops-catalog-aks<\/code>.<\/p>\n<pre><code class=\"bash\">az group create \\\n--name devops-catalog-aks \\\n--location eastus\n<\/code><\/pre>\n<p>Finally, we&#8217;ll create a service principal. To do that, we need to retrieve your subscription ID. It is available through the <code>account list<\/code> command.<\/p>\n<pre><code class=\"bash\">az account list\n<\/code><\/pre>\n<p>You might have one or more accounts. Search for the <code>id<\/code> field in the account you want to use, copy the value, and paste it instead of <code>[...]<\/code> in the command that follows.<\/p>\n<pre><code class=\"bash\">export ARM_SUBSCRIPTION_ID=[...]\n<\/code><\/pre>\n<p>To be on the safe side, we&#8217;ll set the selected subscription to be the currently active one.<\/p>\n<pre><code class=\"bash\">az account set -s $ARM_SUBSCRIPTION_ID\n<\/code><\/pre>\n<p>Now we are finally ready to create a service principal that will be used by Terraform to authenticate.<\/p>\n<pre><code class=\"bash\">export SERVICE_PRINCIPAL=$(\\\naz ad sp create-for-rbac \\\n--role=\"Contributor\" \\\n--scopes=\"\/subscriptions\/$ARM_SUBSCRIPTION_ID\")\n<\/code><\/pre>\n<p>We created the service principal and stored the output inside the <code>SERVICE_PRINCIPAL<\/code> variable. Let&#8217;s take a look at what that output is.<\/p>\n<pre><code class=\"bash\">echo $SERVICE_PRINCIPAL\n<\/code><\/pre>\n<p>The output is as follows.<\/p>\n<pre><code class=\"json\">{\n\"appId\": \"fd4426b0-3304-440d-aa7f-5497bdf79376\",\n\"displayName\": \"azure-cli-2020-09-09-19-57-55\",\n\"name\": \"http:\/\/azure-cli-2020-09-09-19-57-55\",\n\"password\": \"V1g6vcVfMajPRGNolpR0mMt-kbt8KfRA9-\",\n\"tenant\": \"e82964cf-adc2-4086-b7c2-bfbc1601d491\"\n}\n<\/code><\/pre>\n<p>For Terraform to use that service principal, it needs to have those fields stored in environment variables with specific names. That&#8217;s where <code>jq<\/code> comes in. We&#8217;ll use it to extract <code>tenant<\/code>, <code>appId<\/code>, and <code>password<\/code>.<\/p>\n<pre><code class=\"bash\">export ARM_TENANT_ID=$(\necho $SERVICE_PRINCIPAL | \\\njq \".tenant\")\n\nexport ARM_CLIENT_ID=$(\necho $SERVICE_PRINCIPAL | \\\njq \".appId\")\n\nexport ARM_CLIENT_SECRET=$(\necho $SERVICE_PRINCIPAL | \\\njq \".password\")\n<\/code><\/pre>\n<p>Finally, we&#8217;ll store all those variables in a source file so that you do not need to go through the same process every again.<\/p>\n<pre><code class=\"bash\">echo \"export ARM_SUBSCRIPTION_ID=$ARM_SUBSCRIPTION_ID\nexport ARM_TENANT_ID=$ARM_TENANT_ID\nexport ARM_CLIENT_ID=$ARM_CLIENT_ID\nexport ARM_CLIENT_SECRET=$ARM_CLIENT_SECRET\n\" | tee creds\n<\/code><\/pre>\n<p>The next time you need those variables, all you&#8217;ll have to do is execute <code>source creds<\/code>.<\/p>\n<pre><code class=\"bash\">source creds\n<\/code><\/pre>\n<p><em>Do not worry about the possibility of accidentally pushing that file to the Git repo. That could indeed be disastrous. However, I already added <code>\/creds<\/code> to <code>.gitignore<\/code>, so you can feel safe.<\/em><\/p>\n<p>Everything we did was things that everyone would do in Azure, no matter the specific needs. You probably knew all that already.<\/p>\n<p>Now we can move to the part that is specific to Terraform.<\/p>\n<h2>Preparing Terraform Definitions<\/h2>\n<p>Now that we set up the pre-requisite resources in Azure, we can turn our attention towards Terraform.<\/p>\n<p><em>Please note that I will not go into detail on how Terraform works since this is focused on converting Terraform definitions into CD pipelines. So, we&#8217;ll focus only on the relevant parts within the context of Continuous Delivery. If you&#8217;re interested in more details about Terraform, please consult <a href=\"https:\/\/www.devopstoolkitseries.com\/posts\/catalog\/\">The DevOps Toolkit: Catalog, Patterns, And Blueprints<\/a>.<\/em><\/p>\n<p>The most critical and often overlooked part of Terraform is state storage. When used by a single person, and executed manually from a laptop, state storage is not that important. Or, to be more precise, it is not critical to store it somewhere other than on the local file system with, potentially, backing it up somewhere else. But, when working in a team, and, especially, when automating Terraform through pipelines, the location of storage becomes much more important.<\/p>\n<p>I just realized that I &#8220;jumped the gun&#8221;, so let me backtrack a bit.<\/p>\n<p>Terraform must store state about your managed infrastructure and configuration. That state is used by Terraform to map real-world resources to your configuration, to keep track of metadata, and to improve performance for large infrastructures. By default, the state is stored in a local file named <code>terraform.tfstate<\/code>.<\/p>\n<p>In other words, without knowing the state of the resources, Terraform cannot do its job. Every time we execute <code>terraform apply<\/code>, it would think it is the first time we&#8217;re doing that. Many people do not even realize that right away. As long as Terraform is executed from the same directory, the state is there, and its purpose might not be obvious. But, what happens if we run it from a pipeline?<\/p>\n<p>When executing Terraform commands from a pipeline, we cannot make the assumption that the state from previous executions will be present in subsequent builds. When running pipelines from VMs, we cannot be sure that it will always be the same machine. It might, or it might not. It&#8217;s like a lottery. But, when running pipelines from containers, the state will inevitably be gone forever as soon as the container running a pipeline is shut down. We could fix that by mounting persistent storage to each container running pipelines, but that would create other problems like, for example, &#8220;race conditions&#8221;. Fortunately, Terraform has a solution baked int, and we&#8217;ll explore it soon.<\/p>\n<p>The critical thing to understand is that we will need to have a storage bucket. We&#8217;ll see, later on, how to use it. For now, let&#8217;s focus on creating the bucket. In Azure, that means the creation of a storage account and storage container.<\/p>\n<pre><code class=\"bash\">az storage account create \\\n--name devopscatalog \\\n--resource-group devops-catalog-aks \\\n--location eastus \\\n--sku Standard_LRS\n\naz storage container create \\\n--name devopscatalog \\\n--resource-group devops-catalog-aks \\\n--account-name devopscatalog \\\n--public-access blob\n<\/code><\/pre>\n<p>Now that we have the bucket, let&#8217;s take a look at the Terraform definitions I prepared.<\/p>\n<p>We&#8217;ll start with the variables.<\/p>\n<pre><code class=\"bash\">cat variables.tf\n<\/code><\/pre>\n<p>The output is as follows.<\/p>\n<pre><code>variable \"region\" {\ntype = string\ndefault = \"eastus\"\n}\n\nvariable \"resource_group\" {\ntype = string\ndefault = \"devops-catalog-aks\"\n}\n\nvariable \"cluster_name\" {\ntype = string\ndefault = \"docatalog\"\n}\n\nvariable \"dns_prefix\" {\ntype = string\ndefault = \"docatalog\"\n}\n\nvariable \"k8s_version\" {\ntype = string\ndefault = \"CHANGE_VERSION\"\n}\n\nvariable \"min_node_count\" {\ntype = number\ndefault = 3\n}\n\nvariable \"max_node_count\" {\ntype = number\ndefault = 9\n}\n\nvariable \"machine_type\" {\ntype = string\ndefault = \"Standard_D2_v2\"\n}\n\nvariable \"destroy\" {\ntype = bool\ndefault = false\n}\n<\/code><\/pre>\n<p>You can probably guess what each of those variables means from their names. What matters, for now, is that the default value of <code>k8s_version<\/code> is set to <code>CHANGE_VERSION<\/code>. I could not define the AKS version in advance since they are changing all the time, and whatever is the valid one at the time of this writing might not be correct when you&#8217;re working on the exercises. I could have skipped defining the explicit version altogether, but that would be against one of the most essential principles. We always need to be explicit with the versions we&#8217;re using. So, the only alternative left was to force us to change that value to the valid AKS version.<\/p>\n<p>So, let&#8217;s discover the AKS version we will use. We can do that easily through the <code>az<\/code> CLI.<\/p>\n<pre><code class=\"bash\">az aks get-versions --location eastus\n<\/code><\/pre>\n<p>The output, limited to the last entries, is as follows.<\/p>\n<pre><code class=\"json\">{\n\"id\": \"\/subscriptions\/7f9f9b08-7d00-43c9-9d30-f10bb79e9a61\/providers\/Microsoft.ContainerService\/locations\/eastus\/orchestrators\",\n\"name\": \"default\",\n\"orchestrators\": [\n...\n{\n\"default\": null,\n\"isPreview\": null,\n\"orchestratorType\": \"Kubernetes\",\n\"orchestratorVersion\": \"1.17.9\",\n\"upgrades\": [\n{\n\"isPreview\": true,\n\"orchestratorType\": \"Kubernetes\",\n\"orchestratorVersion\": \"1.18.4\"\n},\n{\n\"isPreview\": true,\n\"orchestratorType\": \"Kubernetes\",\n\"orchestratorVersion\": \"1.18.6\"\n}\n]\n},\n{\n\"default\": null,\n\"isPreview\": true,\n\"orchestratorType\": \"Kubernetes\",\n\"orchestratorVersion\": \"1.18.4\",\n\"upgrades\": [\n{\n\"isPreview\": true,\n\"orchestratorType\": \"Kubernetes\",\n\"orchestratorVersion\": \"1.18.6\"\n}\n]\n},\n{\n\"default\": null,\n\"isPreview\": true,\n\"orchestratorType\": \"Kubernetes\",\n\"orchestratorVersion\": \"1.18.6\",\n\"upgrades\": null\n}\n],\n\"type\": \"Microsoft.ContainerService\/locations\/orchestrators\"\n}\n<\/code><\/pre>\n<p>Please choose any of the <code>orchestratorVersion<\/code> (e.g., <code>1.17.9<\/code>). I prefer using the latest stable. That would be the newest that does not have <code>isPreview<\/code> set to <code>true<\/code>. Copy the selected version, and paste it instead of <code>[...]<\/code> in the command that follows.<\/p>\n<pre><code class=\"bash\"># Replace `[...]` with any of the `orchestratorVersion`\nexport VERSION=[...]\n<\/code><\/pre>\n<p>Next, we&#8217;ll use <code>sed<\/code> magic to replace the beforementioned placeholders with the values stored in environment variables.<\/p>\n<pre><code class=\"bash\">cat variables.tf \\\n| sed -e \"s@CHANGE_VERSION@$VERSION@g\" \\\n| tee variables.tf\n<\/code><\/pre>\n<p>Now we can move into <code>main.tf<\/code> that contains the definitions of the resources we&#8217;ll create.<\/p>\n<pre><code class=\"bash\">cat main.tf\n<\/code><\/pre>\n<p>The output is as follows.<\/p>\n<pre><code>provider \"azurerm\" {\nfeatures {}\n}\n\nterraform {\nbackend \"azurerm\" {\nresource_group_name = \"devops-catalog-aks\"\nstorage_account_name = \"devopscatalog\"\ncontainer_name = \"devopscatalog\"\nkey = \"terraform.tfstate\"\n}\n}\n\nresource \"azurerm_kubernetes_cluster\" \"primary\" {\ncount = var.destroy == true ? 0 : 1\nname = var.cluster_name\nlocation = var.region\nresource_group_name = var.resource_group\ndns_prefix = var.dns_prefix\ndefault_node_pool {\nname = var.cluster_name\nvm_size = var.machine_type\nenable_auto_scaling = true\nmax_count = var.max_node_count\nmin_count = var.min_node_count\n}\nidentity {\ntype = \"SystemAssigned\"\n}\n}\n<\/code><\/pre>\n<p>If you are familiar with Terraform, that definition should be straightforward. If you&#8217;re not, this is not the place where we&#8217;ll go into details, so I&#8217;ll just mention what each means.<\/p>\n<p>Through <code>provider \"azurerm\"<\/code>, we are telling Terraform that we want to create and manage Azure resources. We could have stored credentials in that block, but, since we will use environment variables, that is not necessary.<\/p>\n<p>We also have the <code>backend<\/code> set to <code>azurerm<\/code>. That is the signal to Terraform to keep the state in Azure storage instead of storing it locally. As a result, we&#8217;ll be able to manage the resources from any location that has access to that storage and without being constrained to a single machine or some kind of mounted network drives.<\/p>\n<p>Further on, <code>azurerm_kubernetes_cluster<\/code> defines both the control plane and the node pool the cluster will use.<\/p>\n<p>One important thing to note about the <code>azurerm_kubernetes_cluster<\/code> resource is that it has <code>count<\/code> set to <code>var.destroy == true ? 0 : 1<\/code>. The reason behind that is simple. If we apply GitOps principles, everything needs to be defined in Git, and, preferably, in a declarative format. We cannot rely on ad-hoc commands. That is not an issue when creating or updating resources. But, if we&#8217;d like to destroy what we created, we cannot simply remove all the files. Terraform would not allow us to execute <code>terraform apply<\/code> against an empty directory. Even if it would, it might be a good idea to keep the definitions, even if we want to destroy everything. That allows us to be able to change our minds easily without restoring previous commits. Also, there could be quite a few cases when we&#8217;d like to destroy a cluster temporarily and recreate it later. For example, we might have a development cluster that should exist only while we are working.<\/p>\n<p>In any case, I believe that it is easier and more transparent to destroy resources by changing the value of the <code>destroy<\/code> variable, then deleting all the files. So, if we go back <code>count = var.destroy == true ? 0 : 1<\/code>, we can translate it to &#8220;create that resource if <code>destroy<\/code> is set to <code>false<\/code>, or destroy it if it&#8217;s set to <code>true<\/code>.&#8221; We&#8217;ll see that variable in action later.<\/p>\n<p>Finally, the last Terraform file we have is <code>output.tf<\/code>.<\/p>\n<pre><code class=\"bash\">cat output.tf\n<\/code><\/pre>\n<p>The output is as follows.<\/p>\n<pre><code>output \"cluster_name\" {\nvalue = var.cluster_name\n}\n\noutput \"region\" {\nvalue = var.region\n}\n\noutput \"resource_group\" {\nvalue = var.resource_group\n}\n<\/code><\/pre>\n<p>Those are the outputs that we&#8217;ll see every time we <code>apply<\/code> the definitions. Those three might come in handy if anyone wants to know the name of the cluster, the region, or the project ID. You&#8217;ll see their usefulness later when we get to the part of validating the cluster.<\/p>\n<p>We are almost finished with Terraform. The only thing left is to push the changes to Git.<\/p>\n<pre><code class=\"bash\">git add .\n\ngit commit -m \"Initial commit\"\n\ngit push\n<\/code><\/pre>\n<p>Now comes the &#8220;real deal&#8221;. We finally reached the main subject. Everything we did so far was the preparation for the &#8220;big moment&#8221;.<\/p>\n<h2>Defining A Continuous Delivery Pipeline<\/h2>\n<p>We want to have a pipeline that will run every time we push a change to that repository. So, we need to pick a CD tool. It could be any, but not today. Right now, we are going to use <a href=\"https:\/\/codefresh.io\/\">Codefresh.io<\/a>. I won&#8217;t take deep dive into it but focus on the parts that matter when managing infrastructure with Terraform.<\/p>\n<p>Codefresh comes in handy for the task we are about to perform because it is a Software as a Service (SaaS) solution. It could be a self-managed solution, but not inside the same cluster since that would pose a &#8220;chicken and egg&#8221; type of problem. If we need a cluster to manage a cluster, then it cannot be the same one. We&#8217;d need a cluster with a CD platform to manage a different cluster. On top of that, we could not run a pipeline inside the cluster if that pipeline might need to upgrade or even destroy that same cluster. Using a SaaS solution is very handy, especially when dealing with infrastructure.<\/p>\n<p>So, what is the bare minimum for a pipeline that will manage infrastructure using Terraform?<\/p>\n<p>Before answering such a question, I will need to make at least two assumptions.<\/p>\n<p>I will assume that you are creating pull requests (PRs) with proposed changes. Those PRs can be reviewed and, potentially, tested. That means that, as a minimum, we need to be able to see the proposed changes. We need to know how will that PR affect the current state. That means that we probably want to execute <code>terraform plan<\/code> that outputs what will be removed, what will be added, and what will be modified if we <code>apply<\/code> the changes. PRs should NOT apply the changes, at least not inside the same project. In other words, we need to know what the proposed changes are, but not to <code>apply<\/code> them.<\/p>\n<p>The second assumption is that you want to <code>apply<\/code> changes only after merging to the master branch. Any other branch should be ignored.<\/p>\n<p>You might have a different way of working. If that&#8217;s the case, pretend that it is not, and go with the flow. Later on, you should be able to modify the examples to fit whichever process you prefer to use.<\/p>\n<p>With all that in mind, let&#8217;s try to define which steps we might need to have. If we are successful at that, you should have no problems extending them to whatever else you might need.<\/p>\n<p>We can split the needs into two stages; <code>prepare<\/code> and <code>apply<\/code>. Those are arbitrary, and you are free to have steps organized differently, but not today.<\/p>\n<p>In the <code>prepare<\/code> stage, we have to clone the specific revision from the repository.<\/p>\n<p>In the <code>apply<\/code> phase, we have to <code>init<\/code> the project so that the Terraform plugins used in our definitions are downloaded, and the state is retrieved from the bucket. Further on, we already discussed that we need to output the <code>plan<\/code>. We could do that only when working with PRs, but there is no harm in outputting it always. Finally, we need to <code>apply<\/code> the definitions so that the actual state is converged into the desired state. But, as we already discussed, we should <code>apply<\/code> only when making changes to the master branch, and not when working with other branches or with PRs.<\/p>\n<p>Now that we know, more or less, what we want to do, let&#8217;s take a look at one possible implementation of those goals.<\/p>\n<p>Codefresh pipelines are defined as YAML and, by default, are expected to be in <code>codefresh.yaml<\/code> file. I already prepared one, so let&#8217;s take a look.<\/p>\n<pre><code class=\"bash\">cat codefresh.yml\n<\/code><\/pre>\n<p>The output is as follows.<\/p>\n<pre><code class=\"yaml\">version: \"1.0\"\nstages:\n- prepare\n- apply\nsteps:\nmain_clone:\ntitle: Cloning repository\ntype: git-clone\nrepo: \"${{CF_REPO_OWNER}}\/${{CF_REPO_NAME}}\"\nrevision: \"${{CF_BRANCH}}\"\nstage: prepare\ninit:\nimage: hashicorp\/terraform:0.13.0\ntitle: Initializing Terraform\nstage: apply\ncommands:\n- terraform init\nplan:\nimage: hashicorp\/terraform:0.13.0\ntitle: Outputting Terraform plan\nstage: apply\ncommands:\n- terraform plan\napply:\nimage: hashicorp\/terraform:0.13.0\ntitle: Applying Terraform\nstage: apply\ncommands:\n- terraform apply -auto-approve\nwhen:\nbranch:\nonly:\n- master\n<\/code><\/pre>\n<p>That&#8217;s a very simple pipeline. It is split into two stages (<code>prepare<\/code> and <code>apply<\/code>).<\/p>\n<p>Inside the <code>prepare<\/code> stage, we are cloning the <code>revision<\/code> that initiated the build (<code>main_clone<\/code>). There&#8217;s not much more to it, so let&#8217;s move to the steps in the <code>apply<\/code> stage. That&#8217;s where the &#8220;real&#8221; action is happening.<\/p>\n<p>The steps in the second stage (<code>apply<\/code>) should be self-explanatory if you are familiar with Terraform. We are initializing the project through <code>terraform init<\/code> so that the required plugins are downloaded and, more importantly, the state stored in the S3 bucket is retrieved. Further on, we are outputting the plan (<code>plan<\/code>) and applying the definitions (<code>apply<\/code>). Since, by default, <code>terraform apply<\/code> asks for an input confirmation, we are circumventing that through the <code>-auto-approve<\/code> argument.<\/p>\n<p>However, we are not going to execute all those steps always. The flow of events differs depending on whether we are working with a pull request or the <code>master<\/code> branch. To be more precise, when pushing to a pull request, we want to get the information about the changes that will be applied, without changing the actual state. We want the information that we can review and make the decision whether to apply it or not. That&#8217;s why we have the <code>when<\/code> conditional in the <code>apply<\/code> step. It will be executed <code>only<\/code> if the <code>branch<\/code> is <code>master<\/code>.<\/p>\n<p>This process is, in a way, equivalent to executing <code>terraform apply<\/code> manually without <code>-auto-approve<\/code>. If we did that, Terraform would show us all the changes and ask us to type <code>yes<\/code> if we are satisfied with the outcome. However, running <code>terraform apply<\/code> manually prevents us from involving the rest of the team, not to mention that it does not adhere to GitOps principles. By having a pipeline that outputs the <code>plan<\/code> when working with a pull request and <code>apply<\/code> after merging to the master, we can involve the whole team to participate in the review of the changes. We can suggest modifications through comments, approve the changes, and perform all the other steps we usually associate with pull requests. We are, effectively, adopting the same practices as those we might be using when working on an application.<\/p>\n<p>Now, before we continue, there is one crucial thing to note.<\/p>\n<p>Each of those steps is executed in a separate container based on potentially different images. For now, all we need is Terraform, so all the steps, besides <code>main_clone<\/code> are using <code>hashicorp\/terraform<\/code>.<\/p>\n<p>The pipeline should be, more or less, self-explanatory, so let&#8217;s skip further explanations and make it work inside Codefresh.<\/p>\n<h2>Creating And Configuring Codefresh Pipeline<\/h2>\n<p>Let&#8217;s start by opening Codefresh UI in a browser.<\/p>\n<pre><code class=\"bash\">open https:\/\/codefresh.io\/\n<\/code><\/pre>\n<p>Please log in if you are already a Codefresh user, or register if you&#8217;re not. Everything we need can be accomplished through the free plan, so your wallet will not be affected in any form or way, at least not by Codefresh.<\/p>\n<p>Once you log in, you will be presented with the <em>Projects<\/em> screen.<\/p>\n<p>Please create a new project by clicking the <em>CREATE PROJECT<\/em> button if you are a first-time user, or <em>+ NEW PROJECT<\/em> if you already have others.<\/p>\n<p>Type <em>devops-catalog<\/em> as the <em>PROJECT NAME<\/em>, use any <em>PROJECT TAGS<\/em> you like, and select any <em>ICON<\/em>.<\/p>\n<p><img class=\"aligncenter size-large wp-image-17610\" src=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-gke-new-project-583x1024.png\" alt=\"\" width=\"583\" height=\"1024\" srcset=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-gke-new-project-583x1024.png 583w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-gke-new-project-171x300.png 171w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-gke-new-project-11x20.png 11w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-gke-new-project.png 598w\" sizes=\"(max-width: 583px) 100vw, 583px\" \/><\/p>\n<p>Click the <em>CREATE<\/em> button.<\/p>\n<p>Next, we&#8217;ll need to create a new pipeline. As you can surely guess, we can do that by clicking the <em>CREATE PIPELINE<\/em> button.<\/p>\n<p>Type <em>cf-terraform-aks<\/em> as the <em>pipeline name<\/em>, and select <em>cf-terraform-aks<\/em> as the repository. You can use the search field to narrow the list of the repos if you have too many.<\/p>\n<p><img class=\"aligncenter size-large wp-image-17667\" src=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-new-pipeline-821x1024.png\" alt=\"\" width=\"821\" height=\"1024\" srcset=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-new-pipeline-821x1024.png 821w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-new-pipeline-240x300.png 240w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-new-pipeline-768x958.png 768w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-new-pipeline-16x20.png 16w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-new-pipeline.png 1202w\" sizes=\"(max-width: 821px) 100vw, 821px\" \/><\/p>\n<p>Click the <em>CREATE<\/em> button.<\/p>\n<p>Codefresh allows you to specify pipeline definition inside the UI, but, in the majority of cases, it should be stored in a Git repo, just as any other code or config related to the project. We already have <em>codefresh.yaml<\/em>, so there is no good reason not to use it.<\/p>\n<p>Please change <em>Inline YAML<\/em> to <em>Use YAML from Repository<\/em>.<\/p>\n<p>Click the <em>DONE<\/em> button.<\/p>\n<p>As we already discussed, Terraform definitions assume that there are environment variables <code>AWS_ACCESS_KEY_ID<\/code>, <code>AWS_SECRET_ACCESS_KEY<\/code>, and <code>AWS_DEFAULT_REGION<\/code>. We created them locally, in a terminal session. But Codefresh does not know about them. We need to define them there as well.<\/p>\n<p>Please go back to the terminal to output the contents of those variables.<\/p>\n<pre><code class=\"bash\">echo $ARM_SUBSCRIPTION_ID\n\necho $ARM_TENANT_ID\n\necho $ARM_CLIENT_ID\n\necho $ARM_CLIENT_SECRET\n<\/code><\/pre>\n<p>Copy the output of the first (<code>ARM_SUBSCRIPTION_ID<\/code>) and return to Codefresh in your browser.<\/p>\n<p>Select the <em>VARIABLES<\/em> tab, and click the <em>ADD VARIABLE<\/em> button.<\/p>\n<p>Type <em>ARM_SUBSCRIPTION_ID<\/em> as the <em>Key<\/em> and paste the content you copied earlier into the <em>Value<\/em> field.<\/p>\n<p>Finally, we&#8217;ll encrypt the value of that variable by clicking the <em>Encrypt<\/em> button, followed with <em>OK<\/em> to confirm the desire to <em>ENCRYPT VALUE<\/em>.<\/p>\n<p>Now, repeat those same steps two more times to add variables <code>ARM_TENANT_ID<\/code>, <code>ARM_CLIENT_ID<\/code>, and <code>ARM_CLIENT_SECRET<\/code>.<\/p>\n<p><img class=\"aligncenter size-large wp-image-17668\" src=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pipeline-1024x756.png\" alt=\"\" width=\"1024\" height=\"756\" srcset=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pipeline-1024x756.png 1024w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pipeline-300x221.png 300w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pipeline-768x567.png 768w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pipeline-1536x1133.png 1536w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pipeline-2048x1511.png 2048w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pipeline-20x15.png 20w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" \/><\/p>\n<p>Click the <em>SAVE<\/em> button to persist the newly created variable.<\/p>\n<p>We&#8217;re done. We have the pipeline, and the only thing left is to give it a spin.<\/p>\n<h2>Applying Infrastructure Definitions<\/h2>\n<p>Everything is set up, except for our infrastructure. We defined everything we need as a combination of Terraform definitions and a pipeline YAML. From now on, we will not be clicking any buttons to make the &#8220;magic&#8221; happen. Whichever changes we push to the master branch will be applied directly to our infrastructure. Git will make sure to notify Codefresh about the changes of the desired state, and the pipeline will make sure that the actual state is converged to the desired one. Later on, we&#8217;ll see that might not be enough, but, for now, we&#8217;ll focus on the master alone.<\/p>\n<p>However, since we already have the definition stored in Git, we will break the rule and click the <em>RUN<\/em> button ourselves. Hopefully, this will be the first and the last time we&#8217;ll initiate a pipeline from the UI. Think of the action we are about to perform as me showing you what NOT to do.<\/p>\n<p>Please click the <em>RUN<\/em> button, twice.<\/p>\n<p>You will be redirected to the build screen from where you can observe the progress. Feel free to click any of the steps that were already executed or to follow the one that is currently running. You&#8217;ll see the output of the logs.<\/p>\n<p>When the build reaches the <code>apply<\/code> stage steps, it will <code>init<\/code> Terraform by downloading the plugins and restoring the state from the bucket. Further on, it will show the <code>plan<\/code> of the changes that will be performed, and continue to <code>apply<\/code> them.<\/p>\n<p><img class=\"aligncenter size-large wp-image-17669\" src=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-apply-1024x756.png\" alt=\"\" width=\"1024\" height=\"756\" srcset=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-apply-1024x756.png 1024w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-apply-300x221.png 300w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-apply-768x567.png 768w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-apply-1536x1134.png 1536w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-apply-2048x1512.png 2048w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-apply-20x15.png 20w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" \/><\/p>\n<p>A few minutes later, the cluster should have been created, and we can confirm that from a laptop by, let&#8217;s say, executing <code>kubectl get nodes<\/code>. But, to do that, we need to create KubeConfig locally. We can do that through the <code>az aks get-credentials<\/code>, but, for it to work, we need to find out the name of the cluster, the project ID, and the region where it is running. We can get that information in two ways.<\/p>\n<p>One option would be to copy and paste the information we need from the output of the <code>apply<\/code> step. Go ahead. Open the logs of that step (the last one), and you&#8217;ll see the information we need. However, there is a better way which, at the same time, demonstrates the benefits of storing Terraform state in a storage bucket.<\/p>\n<p>Instead of going through the Codefresh UI and digging through build logs, we can restore the Terraform state locally and retrieve all the information we need or, to be more precise, the information defined as <code>output<\/code> values. To do that, we need to <code>init<\/code> the project first.<\/p>\n<pre><code class=\"bash\">terraform init\n<\/code><\/pre>\n<p>That command downloaded the plugins we would need to <code>apply<\/code> the changes. But we are not going to <code>apply<\/code> anything. From now on, applying changes is done through GitOps. We should push changes to Git and let the system converge the actual into the desired state. Besides, that was not the reason why we initialized the project. That command also configured our local project to use the Azure Storage bucket that contains the state. That was the &#8220;real&#8221; reason behind <code>terraform init<\/code>.<\/p>\n<p>Next, we need to synchronize the local state with the one stored in the bucket. We can do that through <code>refresh<\/code>.<\/p>\n<pre><code class=\"bash\">terraform refresh\n<\/code><\/pre>\n<p>The output, limited to the <code>outputs<\/code>, is as follows.<\/p>\n<pre><code>...\nOutputs:\n\ncluster_name = docatalog\nregion = eastus\nresource_group = devops-catalog-aks\n<\/code><\/pre>\n<p>We can see the information we need. Now we could copy it and paste it as arguments to the <code>get-credentials<\/code> command. But we will not do that. The only place worth copying and pasting is <a href=\"https:\/\/stackoverflow.com\/\">Stack Overflow<\/a>. For everything else, it is much better to have self-executable commands that we can easily convert into scripts if needed.<\/p>\n<p>Fortunately, we can use <code>terraform output<\/code> commands to retrieve the information we need. So, without further ado, the commands that will generate the KubeConfig file, which will allow us to interact with the newly created cluster, are as follows.<\/p>\n<pre><code class=\"bash\">export KUBECONFIG=$PWD\/kubeconfig\n\naz aks get-credentials \\\n--name \\\n$(terraform output cluster_name) \\\n--resource-group \\\n$(terraform output resource_group) \\\n--file \\\n$KUBECONFIG\n<\/code><\/pre>\n<p>We defined the environment variable <code>KUBECONFIG<\/code> with the path where we want to store the configuration. That wasn&#8217;t necessary. Without it, the config would be stored in the default location, together with other clusters we might be using. However, that tends to get messy when working with many clusters, so I prefer to have a separate file for each and store it in the same directory where the project with the cluster definitions is.<\/p>\n<p><em>Do not worry about the danger of accidentally pushing <code>kubeconfig<\/code> to the Git repo. It is listed in the <code>.gitignore<\/code> file.<\/em><\/p>\n<p>Further on, we executed the <code>get-credentials<\/code> command. Instead of copying and pasting the name of the cluster, the project ID, and the region, we used <code>terraform output<\/code> commands to retrieve that info.<\/p>\n<p>All that&#8217;s left, before we move on, is to confirm that the cluster is indeed operational.<\/p>\n<pre><code class=\"bash\">kubectl get nodes\n<\/code><\/pre>\n<p>You should see the output of the nodes that constitute the cluster. Hurray!<\/p>\n<p>Nevertheless, we are not yet finished. We still need to figure out how to make the infrastructure changes safe or, at least, less likely to cause damage.<\/p>\n<h2>Incorporating Pull Requests Into Infrastructure Management<\/h2>\n<p>Applying changes to infrastructure directly, without doing any type of review or testing is irresponsible. That might prove to be catastrophic. We could make a mistake that would be poorly reflected on our users (external or internal). Yet, that&#8217;s precisely what we did. So, let me correct the previous statement. Updating existing infra without any review or validation is terrible. Creating infra for the first time is usually OK since no one is yet using it, and we are not updating or destroying existing resources. That&#8217;s why our previous actions were acceptable. We created a new cluster that isn&#8217;t affecting anyone since no one could have been using something that does not exist.<\/p>\n<p>Now, let&#8217;s imagine that we deployed some applications in that cluster and that they are being used by others. How can we make changes to that cluster safely? The answer to that question is in pull requests. That&#8217;s the widely accepted way to propose changes that can be reviewed and tested.<\/p>\n<p>Testing changes proposed through a pull request usually means deployment of an application and some of its dependencies. But, right now, we are not dealing with applications, but with infrastructure. Still, the logic is, more or less, the same. We need to deploy at least some applications so that we can test whether they behave correctly in changed infrastructure.<\/p>\n<p>When compared with PRs related to applications, the additional complication is that we need to create that infrastructure in parallel with the one that we are planning to change, then apply the changes, followed with testing. But, all that is too big of a subject and deserves a separate article. So, we&#8217;ll focus on PRs themselves and reviews and leave the creation of temporary infrastructure that can be used for testing purposes for some other time.<\/p>\n<p>A review of any pull request usually consists of observing the differences in code, communication between team members, and a clear understanding of what would happen if applied to production. We&#8217;ll focus only on the latter, assuming that you already know how to review code changes and communicate with your team through comments or other means.<\/p>\n<p>The good news is that we already set up almost everything we need. The <code>plan<\/code> step inside the <code>apply<\/code> stage is executing <code>terraform plan<\/code> that shows which changes will be performed if we execute <code>terraform apply<\/code>. We can use that output to evaluate whether we should proceed with the proposed changes.<\/p>\n<p>On the other hand, we do NOT want to execute <code>terraform apply<\/code> as a result of creating a pull request. That would defy the purpose of proposing changes.<\/p>\n<p>If you take another look at the <code>apply<\/code> step in <code>codefresh.yml<\/code>, you&#8217;ll see the following declaration.<\/p>\n<pre><code class=\"yaml\">...\napply:\n...\nwhen:\nbranch:\nonly:\n- master\n<\/code><\/pre>\n<p>That is a conditional statement, and you can probably guess that it means that the step should be executed <code>only<\/code> if the <code>branch<\/code> is <code>master<\/code>.<\/p>\n<p>In other words, if we make a change to the master branch, all the steps will be executed, including the one that applies the changes. But, in all other cases (including PRs), the <code>apply<\/code> step will be skipped.<\/p>\n<p>So, we are all set, except for one tiny detail. We need to modify Codefresh triggers so that pipelines are executed only if a change is made to the master branch, or when creating or making changes to PRs. That means that we need to do two things. We need to restrict Codefresh to trigger pipelines only when changes are made to the master branch and ignore others. On top of that, we need to add an additional trigger that will run builds whenever we create a new pull request or make changes to an existing one.<\/p>\n<p>Please go back to the Codefresh UI in your browser.<\/p>\n<p>You should see the breadcrumbs near the top of the screen. Click the <em>cf-terraform-aks<\/em> link.<\/p>\n<p>Next, we want to change the triggers, so click the <em>TRIGGERS<\/em> tab. You&#8217;ll see a single trigger created by default when we created the pipeline. Click the edit button next to it.<\/p>\n<p>For now, we&#8217;ll focus on making sure that only the master branch triggers pipeline builds.<\/p>\n<p>Please change the <em>BRANCH (REGEX EXPRESSION)<\/em> to <em>\/master\/gi<\/em>. That will ensure that only the changes to the master branch will trigger pipelines.<\/p>\n<p>Click the <em>UPDATE<\/em> button.<\/p>\n<p>Next, we need to add an additional trigger that will handle creation and changes to pull requests.<\/p>\n<p>Click <em>ADD TRIGGER<\/em>, followed with the <em>+ ADD TRIGGER<\/em> button. Select <em>GIT<\/em> as the type. Click the <em>NEXT<\/em> button.<\/p>\n<p>Change the <em>TRIGGER NAME<\/em> to <em>pr<\/em>, or whatever you would like to call it. Select the <em>cf-terraform-aks<\/em> repository.<\/p>\n<p>For this trigger, we are not interested in the push commits coming from branches since we already have that set up in the other trigger. So, unselect the <em>TRIGGER BY<\/em> option <em>Push commits<\/em>.<\/p>\n<p>Select the <em>TRIGGER BY<\/em> options <em>Pull request opened<\/em> and <em>Pull request synchronized<\/em>. The names should be self-explanatory.<\/p>\n<p>Now, if we leave it as-is, the trigger would run pipelines on any pull request. But we might not want that. We might want to limit them only to PRs created against the master branch.<\/p>\n<p><em>I don&#8217;t think I ever created a PR that is not against the master branch, but one can never know how far the creativity of others goes.<\/em><\/p>\n<p>So, we&#8217;ll change the <em>PULL REQUEST TARGET BRANCH (REGEX EXPRESSION)<\/em> to <em>\/master\/gi<\/em><\/p>\n<p>Click <em>NEXT<\/em>, followed by the <em>DONE<\/em> button.<\/p>\n<p><img class=\"aligncenter size-full wp-image-17614\" src=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-gke-triggers.png\" alt=\"\" width=\"1005\" height=\"405\" srcset=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-gke-triggers.png 1005w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-gke-triggers-300x121.png 300w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-gke-triggers-768x309.png 768w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-gke-triggers-20x8.png 20w\" sizes=\"(max-width: 1005px) 100vw, 1005px\" \/><\/p>\n<p>Close the dialog by pressing the <em>X<\/em> icon in the top-left corner of the <em>Pipeline triggers<\/em> dialog.<br \/>\n!<br \/>\nNow we&#8217;re ready to give pull requests a spin.<\/p>\n<h2>Using Pull Requests To Preview Changes To Infrastructure<\/h2>\n<p>Let&#8217;s create some changes to our infrastructure and see how we can benefit from pull requests and triggers to Codefresh pipelines.<\/p>\n<p>The first few steps are typical for creating any pull request. It starts with the creation of a new branch.<\/p>\n<pre><code class=\"bash\">git checkout -b destroy\n<\/code><\/pre>\n<p>We could make some changes to the definition of the infrastructure. We could, for example, upgrade the Kubernetes version. Or, we could add another node pool. But, since we are getting close to the end of the story, we might just as well destroy the whole cluster. I don&#8217;t want you to blame me for the high cost of using my examples, so I always end tutorials with the destruction of everything. This will not be an exception, except that we&#8217;ll proceed with the destruction before reaching the end.<\/p>\n<p>The typical way to destroy the resources created through Terraform is to execute <code>terraform destroy<\/code>. But that command is not very &#8220;friendly&#8221; to CD processes. It makes much more sense to keep using <code>terraform apply<\/code>, no matter whether we want to create, update, or destroy resources.<\/p>\n<p>However, there is no intuitive way to do something like that in Terraform. If, for example, we delete all the files, <code>terraform apply<\/code> would think that there is something fishy about it and prevent us from proceeding. On top of that, deleting all the files from Git might complicate tracking and reviews. Similarly, sometimes we might want to delete resources but still keep the option to recreate them later. That is especially true when dealing with those that are temporary like, for example, a cluster used as a development environment. One might create it at the beginning of the workday and shut it down when finished working.<\/p>\n<p>We can use the <code>count<\/code> property available in all Terraform resources. It can be set to <code>1<\/code> by default and changed to <code>0<\/code> if we want to destroy that resource. To make it a bit more user friendly, we can have a boolean variable to control that behavior.<\/p>\n<p>That&#8217;s why the current definition has the variable <code>destroy<\/code> of type <code>bool<\/code> set, by <code>default<\/code>, to <code>false<\/code>. Further on, we have <code>count = var.destroy == true ? 0 : 1<\/code> set to all the relevant resources. We saw those when we explored <code>variables.tf<\/code> and <code>main.tf<\/code>. Feel free to take another look at those files if your memory does not serve you, or if you did not pay attention.<\/p>\n<p>Now that the logic behind destructive tendencies is a bit clearer, let&#8217;s create a PR that will propose the destruction of the whole cluster.<\/p>\n<p>Please open <em>variables.tf<\/em> in your favorite editor and <strong>change the value of the <code>destroy<\/code> variable to <code>true<\/code><\/strong>. Make sure to save the changes.<\/p>\n<p>Now that we modified the Terraform definition, we can push the changes to the newly created branch.<\/p>\n<pre><code class=\"bash\">git add .\n\ngit commit -m \"Destroying everything\"\n\ngit push \\\n--set-upstream origin destroy\n<\/code><\/pre>\n<p>Feel free to go back to Codefresh. If you do, you&#8217;ll see that a pipeline build was not triggered by that push. That was expected since we modified the triggers to work only with the master branch and pull requests.<\/p>\n<p>Next, we&#8217;ll pretend that we are finished working with that branch and that we are ready to propose that those changes be applied to production. So, as you already know, it is time to create a pull request.<\/p>\n<pre><code class=\"bash\">open https:\/\/github.com\/$GH_ORG\/cf-terraform-aks\n<\/code><\/pre>\n<p>Please create a pull request. I&#8217;m sure you already know how to do that, so I will not give you step-by-step instructions. If you don&#8217;t know how to do it, the first step is to feel ashamed for a few minutes and then Google it.<\/p>\n<p>Now, go back to Codefresh UI opened in your browser, and observe that a new pipeline build was triggered. The last two steps are what matters in the context of pull requests.<\/p>\n<p>The logs from the second to last step (the one named <em>Outputting Terraform plan<\/em>) should show what will happen if we <code>apply<\/code> the changes. In this case, you will see that some of the resources will be deleted.<\/p>\n<p>The second important observation is that the <em>Applying Terraform<\/em> step was skipped. We did not <code>apply<\/code> the changes, but only executed <code>terraform plan<\/code> that shows us what would happen if we do.<\/p>\n<p><img class=\"aligncenter size-large wp-image-17670\" src=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pr-build-1024x759.png\" alt=\"\" width=\"1024\" height=\"759\" srcset=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pr-build-1024x759.png 1024w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pr-build-300x222.png 300w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pr-build-768x569.png 768w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pr-build-1536x1138.png 1536w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pr-build-2048x1518.png 2048w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/cf-aks-pr-build-20x15.png 20w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" \/><\/p>\n<p>Further on, we would probably review the code of the proposed code changes, discuss it with our peers, ask for someone to approve, and do all the other things we usually do with PRs. We&#8217;ll imagine that we did all that,<\/p>\n<p>All that is left is to confirm that we indeed want to <code>apply<\/code> the changes that will destroy the cluster. Since we are enforcing the GitOps principles, the way to proceed is to merge the PR and let the machines do the rest. So, please go back to the pull request and hit the <em>Merge pull request<\/em> button.<\/p>\n<p>We&#8217;re finished with that branch, so let&#8217;s go back to <code>master<\/code>.<\/p>\n<pre><code class=\"bash\">git checkout master\n<\/code><\/pre>\n<p>Go back to Codefresh. You&#8217;ll see that a new pipeline build was triggered. It will complete soon. When it&#8217;s done, your cluster will be gone. If you do not believe me, open the Azure Portal and confirm that it is no more.<\/p>\n<p>From now on, you can create or destroy the cluster by changing the value of the variable <code>destroy<\/code>. Of course, that does not mean those are the only operations you can do. You can add new resources, change the properties of the existing ones, or do (almost) anything else related to your infrastructure.<\/p>\n<p>What matters is that your job is to write or modify Terraform definitions and push them to Git. The machines will do the rest.<\/p>\n<h2>What Are We Missing?<\/h2>\n<p>We could have done many other things, but due to constraints of time and space, we didn&#8217;t. We could create a temporary parallel infrastructure for testing purposes. We could add tests that would validate changes. We could send the output of <code>terraform plan<\/code> to the PR to have all the information there. We could also employ ChatOps to further streamline the whole process.<\/p>\n<p>Time and space are limited, and this article is already much longer than I initially thought it will be. So, we&#8217;ll end here, for now.<\/p>\n<input class=\"fooboxshare_post_id\" type=\"hidden\" value=\"17666\"\/>","protected":false},"excerpt":{"rendered":"<p>There are many articles and videos about practicing Continuous Delivery (CD) with applications, but not nearly as many for infrastructure. The same can be said for GitOps applied to infrastructure. That is a bit strange given that applications and infrastructure are almost the same today. Both are defined as code, and everyone stores code in &hellip; <a href=\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/\">Read more<\/a><\/p>\n","protected":false},"author":125,"featured_media":17672,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[3120,5703,1657],"tags":[44,411,412,651,3160,4321,5504,5505],"yoast_head":"<!-- This site is optimized with the Yoast SEO Premium plugin v17.9 (Yoast SEO v18.4.1) - https:\/\/yoast.com\/wordpress\/plugins\/seo\/ -->\n<title>Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS) | Codefresh<\/title>\n<meta name=\"robots\" content=\"index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\" \/>\n<link rel=\"canonical\" href=\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/\" \/>\n<meta property=\"og:locale\" content=\"en_US\" \/>\n<meta property=\"og:type\" content=\"article\" \/>\n<meta property=\"og:title\" content=\"Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS)\" \/>\n<meta property=\"og:description\" content=\"There are many articles and videos about practicing Continuous Delivery (CD) with applications, but not nearly as many for infrastructure. The same can be said for GitOps applied to infrastructure. That is a bit strange given that applications and infrastructure are almost the same today. Both are defined as code, and everyone stores code in &hellip; Read more\" \/>\n<meta property=\"og:url\" content=\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/\" \/>\n<meta property=\"og:site_name\" content=\"Codefresh\" \/>\n<meta property=\"article:publisher\" content=\"https:\/\/www.facebook.com\/codefresh.io\" \/>\n<meta property=\"article:published_time\" content=\"2020-10-15T11:23:00+00:00\" \/>\n<meta property=\"article:modified_time\" content=\"2021-04-19T11:01:23+00:00\" \/>\n<meta property=\"og:image\" content=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/Terraform-and-codefresh.png\" \/>\n\t<meta property=\"og:image:width\" content=\"1024\" \/>\n\t<meta property=\"og:image:height\" content=\"512\" \/>\n\t<meta property=\"og:image:type\" content=\"image\/png\" \/>\n<meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n<meta name=\"twitter:creator\" content=\"@codefresh\" \/>\n<meta name=\"twitter:site\" content=\"@codefresh\" \/>\n<meta name=\"twitter:label1\" content=\"Written by\" \/>\n\t<meta name=\"twitter:data1\" content=\"Contributor\" \/>\n\t<meta name=\"twitter:label2\" content=\"Est. reading time\" \/>\n\t<meta name=\"twitter:data2\" content=\"37 minutes\" \/>\n<script type=\"application\/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https:\/\/schema.org\",\"@graph\":[{\"@type\":\"Organization\",\"@id\":\"https:\/\/codefresh.io\/#organization\",\"name\":\"Codefresh\",\"url\":\"https:\/\/codefresh.io\/\",\"sameAs\":[\"https:\/\/www.facebook.com\/codefresh.io\",\"https:\/\/www.linkedin.com\/company\/codefresh\",\"https:\/\/www.youtube.com\/channel\/UC9r94SY6BqN05kXPIHsDXPg\",\"https:\/\/twitter.com\/codefresh\"],\"logo\":{\"@type\":\"ImageObject\",\"@id\":\"https:\/\/codefresh.io\/#logo\",\"inLanguage\":\"en-US\",\"url\":\"https:\/\/codefresh.io\/wp-content\/uploads\/2022\/02\/Codefresh_Logo_Vertical_LightBkgd.png\",\"contentUrl\":\"https:\/\/codefresh.io\/wp-content\/uploads\/2022\/02\/Codefresh_Logo_Vertical_LightBkgd.png\",\"width\":800,\"height\":800,\"caption\":\"Codefresh\"},\"image\":{\"@id\":\"https:\/\/codefresh.io\/#logo\"}},{\"@type\":\"WebSite\",\"@id\":\"https:\/\/codefresh.io\/#website\",\"url\":\"https:\/\/codefresh.io\/\",\"name\":\"Codefresh\",\"description\":\"\",\"publisher\":{\"@id\":\"https:\/\/codefresh.io\/#organization\"},\"potentialAction\":[{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https:\/\/codefresh.io\/?s={search_term_string}\"},\"query-input\":\"required name=search_term_string\"}],\"inLanguage\":\"en-US\"},{\"@type\":\"ImageObject\",\"@id\":\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#primaryimage\",\"inLanguage\":\"en-US\",\"url\":\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/Terraform-and-codefresh.png\",\"contentUrl\":\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/Terraform-and-codefresh.png\",\"width\":1024,\"height\":512},{\"@type\":\"WebPage\",\"@id\":\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#webpage\",\"url\":\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/\",\"name\":\"Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS) | Codefresh\",\"isPartOf\":{\"@id\":\"https:\/\/codefresh.io\/#website\"},\"primaryImageOfPage\":{\"@id\":\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#primaryimage\"},\"datePublished\":\"2020-10-15T11:23:00+00:00\",\"dateModified\":\"2021-04-19T11:01:23+00:00\",\"breadcrumb\":{\"@id\":\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#breadcrumb\"},\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"ReadAction\",\"target\":[\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/\"]}]},{\"@type\":\"BreadcrumbList\",\"@id\":\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#breadcrumb\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Home\",\"item\":\"https:\/\/codefresh.io\/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS)\"}]},{\"@type\":\"Article\",\"@id\":\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#article\",\"isPartOf\":{\"@id\":\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#webpage\"},\"author\":{\"@id\":\"https:\/\/codefresh.io\/#\/schema\/person\/268e5c2e4740502fae6d803783a16c75\"},\"headline\":\"Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS)\",\"datePublished\":\"2020-10-15T11:23:00+00:00\",\"dateModified\":\"2021-04-19T11:01:23+00:00\",\"mainEntityOfPage\":{\"@id\":\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#webpage\"},\"wordCount\":6501,\"commentCount\":0,\"publisher\":{\"@id\":\"https:\/\/codefresh.io\/#organization\"},\"image\":{\"@id\":\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#primaryimage\"},\"thumbnailUrl\":\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/Terraform-and-codefresh.png\",\"keywords\":[\"Kubernetes\",\"azure\",\"microsoft\",\"K8s\",\"aks\",\"Terraform\",\"Microsoft Azure\",\"Azure Kubernetes Service\"],\"articleSection\":[\"Continuous Deployment\/Delivery\",\"GitOps\",\"Kubernetes Tutorials\"],\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"CommentAction\",\"name\":\"Comment\",\"target\":[\"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#respond\"]}]},{\"@type\":\"Person\",\"@id\":\"https:\/\/codefresh.io\/#\/schema\/person\/268e5c2e4740502fae6d803783a16c75\",\"name\":\"Contributor\",\"image\":{\"@type\":\"ImageObject\",\"@id\":\"https:\/\/codefresh.io\/#personlogo\",\"inLanguage\":\"en-US\",\"url\":\"https:\/\/secure.gravatar.com\/avatar\/26bd6dd87bf70f0c1a44721c8b3abbbd?s=96&d=blank&r=g\",\"contentUrl\":\"https:\/\/secure.gravatar.com\/avatar\/26bd6dd87bf70f0c1a44721c8b3abbbd?s=96&d=blank&r=g\",\"caption\":\"Contributor\"},\"url\":\"https:\/\/codefresh.io\/author\/contributor\/\"}]}<\/script>\n<!-- \/ Yoast SEO Premium plugin. -->","yoast_head_json":{"title":"Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS) | Codefresh","robots":{"index":"index","follow":"follow","max-snippet":"max-snippet:-1","max-image-preview":"max-image-preview:large","max-video-preview":"max-video-preview:-1"},"canonical":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/","og_locale":"en_US","og_type":"article","og_title":"Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS)","og_description":"There are many articles and videos about practicing Continuous Delivery (CD) with applications, but not nearly as many for infrastructure. The same can be said for GitOps applied to infrastructure. That is a bit strange given that applications and infrastructure are almost the same today. Both are defined as code, and everyone stores code in &hellip; Read more","og_url":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/","og_site_name":"Codefresh","article_publisher":"https:\/\/www.facebook.com\/codefresh.io","article_published_time":"2020-10-15T11:23:00+00:00","article_modified_time":"2021-04-19T11:01:23+00:00","og_image":[{"width":1024,"height":512,"url":"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/Terraform-and-codefresh.png","type":"image\/png"}],"twitter_card":"summary_large_image","twitter_creator":"@codefresh","twitter_site":"@codefresh","twitter_misc":{"Written by":"Contributor","Est. reading time":"37 minutes"},"schema":{"@context":"https:\/\/schema.org","@graph":[{"@type":"Organization","@id":"https:\/\/codefresh.io\/#organization","name":"Codefresh","url":"https:\/\/codefresh.io\/","sameAs":["https:\/\/www.facebook.com\/codefresh.io","https:\/\/www.linkedin.com\/company\/codefresh","https:\/\/www.youtube.com\/channel\/UC9r94SY6BqN05kXPIHsDXPg","https:\/\/twitter.com\/codefresh"],"logo":{"@type":"ImageObject","@id":"https:\/\/codefresh.io\/#logo","inLanguage":"en-US","url":"https:\/\/codefresh.io\/wp-content\/uploads\/2022\/02\/Codefresh_Logo_Vertical_LightBkgd.png","contentUrl":"https:\/\/codefresh.io\/wp-content\/uploads\/2022\/02\/Codefresh_Logo_Vertical_LightBkgd.png","width":800,"height":800,"caption":"Codefresh"},"image":{"@id":"https:\/\/codefresh.io\/#logo"}},{"@type":"WebSite","@id":"https:\/\/codefresh.io\/#website","url":"https:\/\/codefresh.io\/","name":"Codefresh","description":"","publisher":{"@id":"https:\/\/codefresh.io\/#organization"},"potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https:\/\/codefresh.io\/?s={search_term_string}"},"query-input":"required name=search_term_string"}],"inLanguage":"en-US"},{"@type":"ImageObject","@id":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#primaryimage","inLanguage":"en-US","url":"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/Terraform-and-codefresh.png","contentUrl":"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/Terraform-and-codefresh.png","width":1024,"height":512},{"@type":"WebPage","@id":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#webpage","url":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/","name":"Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS) | Codefresh","isPartOf":{"@id":"https:\/\/codefresh.io\/#website"},"primaryImageOfPage":{"@id":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#primaryimage"},"datePublished":"2020-10-15T11:23:00+00:00","dateModified":"2021-04-19T11:01:23+00:00","breadcrumb":{"@id":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#breadcrumb"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/"]}]},{"@type":"BreadcrumbList","@id":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/codefresh.io\/"},{"@type":"ListItem","position":2,"name":"Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS)"}]},{"@type":"Article","@id":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#article","isPartOf":{"@id":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#webpage"},"author":{"@id":"https:\/\/codefresh.io\/#\/schema\/person\/268e5c2e4740502fae6d803783a16c75"},"headline":"Applying Gitops and Continuous Delivery (CD) on Infrastructure Using Terraform, Codefresh, and Azure Kubernetes Service (AKS)","datePublished":"2020-10-15T11:23:00+00:00","dateModified":"2021-04-19T11:01:23+00:00","mainEntityOfPage":{"@id":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#webpage"},"wordCount":6501,"commentCount":0,"publisher":{"@id":"https:\/\/codefresh.io\/#organization"},"image":{"@id":"https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#primaryimage"},"thumbnailUrl":"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/Terraform-and-codefresh.png","keywords":["Kubernetes","azure","microsoft","K8s","aks","Terraform","Microsoft Azure","Azure Kubernetes Service"],"articleSection":["Continuous Deployment\/Delivery","GitOps","Kubernetes Tutorials"],"inLanguage":"en-US","potentialAction":[{"@type":"CommentAction","name":"Comment","target":["https:\/\/codefresh.io\/about-gitops\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/#respond"]}]},{"@type":"Person","@id":"https:\/\/codefresh.io\/#\/schema\/person\/268e5c2e4740502fae6d803783a16c75","name":"Contributor","image":{"@type":"ImageObject","@id":"https:\/\/codefresh.io\/#personlogo","inLanguage":"en-US","url":"https:\/\/secure.gravatar.com\/avatar\/26bd6dd87bf70f0c1a44721c8b3abbbd?s=96&d=blank&r=g","contentUrl":"https:\/\/secure.gravatar.com\/avatar\/26bd6dd87bf70f0c1a44721c8b3abbbd?s=96&d=blank&r=g","caption":"Contributor"},"url":"https:\/\/codefresh.io\/author\/contributor\/"}]}},"acf":[],"_links":{"self":[{"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/posts\/17666"}],"collection":[{"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/users\/125"}],"replies":[{"embeddable":true,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/comments?post=17666"}],"version-history":[{"count":0,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/posts\/17666\/revisions"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/media\/17672"}],"wp:attachment":[{"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/media?parent=17666"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/categories?post=17666"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/tags?post=17666"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}