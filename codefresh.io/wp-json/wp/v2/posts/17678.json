{"id":17678,"date":"2020-10-19T10:01:43","date_gmt":"2020-10-19T18:01:43","guid":{"rendered":"https:\/\/codefresh.io\/?p=17678"},"modified":"2021-01-08T08:23:17","modified_gmt":"2021-01-08T16:23:17","slug":"installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks","status":"publish","type":"post","link":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/","title":{"rendered":"How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS"},"content":{"rendered":"<p>We are about to install and manage Argo CD through a CD pipeline.<\/p>\n<p><em>&#8220;Why would we do that? We can just as well accomplish that through a command like <code>kubectl apply<\/code> or <code>helm upgrade --install<\/code>.&#8221;<\/em><\/p>\n<p>I&#8217;m glad you asked.<\/p>\n<p>The primary objective of Argo CD is to help us apply GitOps processes when deploying applications. It is directing us towards the world in which everything is defined as code, and all code is stored in Git. Once we set it up, we will be able to manage all our applications without ever executing any command from a terminal. We could completely remove any access to the control plane while still being able to deploy as frequently as possible. With GitOps, Git becomes the barrier between humans and machines. We push changes to Git, and machines are applying those changes. We are defining our desires, and machines are converging the actual into the desired state.<\/p>\n<p><em>If you&#8217;re not already familiar with GitOps, please watch <a href=\"https:\/\/youtu.be\/qwyRJlmG5ew\">What Is GitOps And Why Do We Want It?<\/a> for a brief overview. Similarly, if you are new to Argo CD, you should get a quick hands-on introduction through the <a href=\"https:\/\/youtu.be\/vpWQeoaiRM4\">Argo CD: Applying GitOps Principles To Manage Production Environment In Kubernetes<\/a> video.<\/em><\/p>\n<p>The problem is that we cannot use Argo CD to apply GitOps-style deployments without deploying Argo CD itself. It&#8217;s a chicken and egg type of problem. Without Argo CD, we have to deploy applications through commands like <code>kubectl<\/code> and <code>helm install<\/code>. Yet, if we are to adhere to the GitOps principles that Argo CD promotes, we shouldn&#8217;t be running such commands manually from a terminal. All that means that we can use Argo CD for all our deployments, except for the installation and management of Argo CD itself. So, we have to resort to a different tool to manage Argo CD definition stored in Git.<\/p>\n<p>What we can do is define a CD pipeline that will deploy and manage Argo CD. Nevertheless, that would result in the same &#8220;chicken and egg&#8221; problem. Who is going to deploy a CD solution in a way that it follows GitOps principles? The short answer is &#8220;nobody&#8221;. We&#8217;ll use <a href=\"codhttps:\/\/codefresh.io\/codefresh-signup\/?utm_source=Blog&amp;utm_medium=Post&amp;utm_campaign=vfarcic-argocd-install\">Codefresh<\/a>, which happens to be SaaS, even though you could run it in the self-managed mode. It&#8217;s already running, and all we have to do is notify it that there is a Git repository with a pipeline that will deploy and manage Argo CD.<\/p>\n<p>We&#8217;ll combine Codefresh, Terraform, <code>kubectl<\/code>, <code>helm<\/code>, and a bit of custom scripting, and envelop all that into a Codefresh pipeline. If we are successful, this might be the last time you, your colleagues, your pipelines, or any other human or a machine executes <code>kubectl<\/code>, <code>helm<\/code>, or similar commands. As a matter of fact, when we are finished, you should be able to remove any Ingress traffic to the control plane (to Kube API). You will be able to prevent both people and other applications from accessing it in any form or way except, maybe, in the read-only mode. Isn&#8217;t that a worthy goal?<\/p>\n<p>Let&#8217;s get going.<\/p>\n<h2>Setting Up The Scene<\/h2>\n<p>We need to set up a few requirements.<\/p>\n<p>To begin with, we&#8217;ll need Codefresh CLI. If you are already using <a href=\"https:\/\/codefresh.io\/codefresh-signup\/?utm_source=Blog&amp;utm_medium=Post&amp;utm_campaign=vfarcic-argocd-install\">codefresh.io<\/a>, you might be used to doing everything through the UI. Not today. We&#8217;ll use the CLI for the few operations we&#8217;ll need to do in Codefresh.<\/p>\n<p>Please follow the instructions from the <a href=\"https:\/\/codefresh-io.github.io\/cli\/installation\/\">Codefresh CLI Installation<\/a> page. Once the CLI is installed, you should authenticate it. You&#8217;ll need an API key for that. If you do not have it already, go to the <a href=\"https:\/\/g.codefresh.io\/user\/settings\">User Settings<\/a> page and click the <em>GENERATE<\/em> button inside the <em>API Keys<\/em> section. Type <em>devops-catalog<\/em> as the <em>KEY NAME<\/em>, select the <em>SCOPES<\/em> checkbox, and click the <em>CREATE<\/em> button. Make sure to copy it by clicking the <em>Copy token to clipboard<\/em> link below the <em>API KEY<\/em> field.<\/p>\n<p><em>All the commands are available in the <a href=\"https:\/\/gist.github.com\/050dfd04db243f0344093fbaa85f39be\">deploy-argo-cf.sh<\/a> Gist. Feel free to use it if you&#8217;re too lazy to type. There&#8217;s no shame in copy &amp; paste.<\/em><\/p>\n<p>Execute the command that follows once you have the token.<\/p>\n<p><em>Please replace <code>[...]<\/code> with the token you just copied to the clipboard.<\/em><\/p>\n<pre><code class=\"bash\">codefresh auth \\\n    create-context devops-catalog \\\n    --api-key [...]\n<\/code><\/pre>\n<p><em>If you are a Windows user, I will assume that you are running the commands from a Bourne Again Shell (Bash) or a Z Shell (Zsh) and not PowerShell. That should not be a problem if you followed the instructions on setting up Windows Subsystem for Linux (WSL) explained in the <a href=\"https:\/\/youtu.be\/zSqugKeOa1Y\">Installing Windows Subsystem For Linux (WSL)<\/a> YouTube video. If you do not like WSL, a Bash emulator like GitBash should do. If none of those is an acceptable option, you might need to modify some of the commands in the examples that follow.<\/em><\/p>\n<p>Next, we&#8217;ll need a Kubernetes cluster. However, it cannot be any cluster since the examples assume that you followed the instructions presented in my previous article. Please make sure that you went through them, or be prepared to tweak the examples. You can find the instructions in any of the following articles.<\/p>\n<ul>\n<li><a href=\"https:\/\/codefresh.io\/continuous-deployment\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-google-kubernetes-engine-gke\/\">Applying GitOps And Continuous Delivery (CD) On Infrastructure Using Terraform, Codefresh, And Google Kubernetes Engine (GKE)<\/a><\/li>\n<li><a href=\"https:\/\/codefresh.io\/continuous-deployment\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-aws-elastic-kubernetes-service-eks\/\">Applying GitOps And Continuous Delivery (CD) On Infrastructure Using Terraform, Codefresh, And AWS Elastic Kubernetes Service (EKS)<br \/>\n<\/a><\/li>\n<li><a href=\"https:\/\/codefresh.io\/kubernetes-tutorial\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/\">Applying GitOps And Continuous Delivery (CD) On Infrastructure Using Terraform, Codefresh, And Azure Kubernetes Service (AKS)<\/a><\/li>\n<\/ul>\n<p>If you destroyed the cluster after you finished reading that article (as you should have), I prepared a Gist with the instructions on how to recreate it. Please follow the <code>Create A Cluster<\/code> section of the Gist related to your favorite Kubernetes distribution.<\/p>\n<ul>\n<li>Google Kubernetes Engine (GKE): <a href=\"https:\/\/gist.github.com\/0f8ba1ee61d0fe08f4d9290a17145440\">gke-codefresh.sh<\/a><\/li>\n<li>AWS Elastic Kubernetes Service (EKS): <a href=\"https:\/\/gist.github.com\/ddfa70d992a0a92ba63b591e72aaa6a6\">eks-codefresh.sh<\/a><\/li>\n<li>Azure Kubernetes Service (AKS): <a href=\"https:\/\/gist.github.com\/fc32da271b7648e3db6e040401353cc5\">aks-codefresh.sh<\/a><\/li>\n<\/ul>\n<p>Now we should have a Kubernetes cluster up and running, and all we did was change a single value in Terraform and let Codefresh take care of executing the steps required to make the magic happen.<\/p>\n<p><em>If that was a &#8220;real world&#8221; situation, we should have created a pull request with proposed changes, review it, test it, and merge it to master. But this is a demo, so shortcuts are allowed.<\/em><\/p>\n<p>We might want to be able to access the newly created cluster from our laptop. Typically, you wouldn&#8217;t need that once you start trusting your GitOps processes and automation. Nevertheless, being able to connect to it will be useful for our exercises, so we&#8217;ll need to retrieve the Kube config of the new cluster. At the same time, that will give us an insight into one of the steps we&#8217;ll need to add to our pipeline. Later on, you&#8217;ll see why that extension is necessary, but, for now, let&#8217;s focus on how to retrieve the config locally.<\/p>\n<p>I already created a script that will do just that, and it is located in the same repo we&#8217;re using to create and manage the cluster. Let&#8217;s go there.<\/p>\n<p><em>Please replace <code>[...]<\/code> in the commands that follow with your Kubernetes platform. Use <code>gke<\/code>, <code>eks<\/code>, or <code>aks<\/code> as the value.<\/em><\/p>\n<pre><code class=\"bash\"># Replace `[...]` with the k8s platform (e.g., `gke`, `eks`, or `aks`)\nexport K8S_PLATFORM=[...]\n\ncd cf-terraform-$K8S_PLATFORM\n<\/code><\/pre>\n<p>To connect to the cluster, we&#8217;ll need to retrieve some information like, for example, the name of the cluster. Since Terraform created it, we can use its output values to get the info we need. Given that the Terraform state is stored in the remote storage, the first step is to initialize the project so that the local copy becomes aware of it, and download the plugins it might need.<\/p>\n<pre><code class=\"bash\">terraform init\n<\/code><\/pre>\n<p>The commands to retrieve Kube config differ from one provider to another. So, instead of going through all the variations, I prepared a script that will execute the required commands to create <code>kubeconfig.yaml<\/code>. Let&#8217;s take a look at it.<\/p>\n<pre><code class=\"bash\">cat get-kubeconfig.sh\n<\/code><\/pre>\n<p>As I already mentioned, the content of that script differs from one provider to another. In the interest of brevity, I&#8217;ll skip explaining the differences in the way we retrieve the config for EKS, AKS, and GKE. I&#8217;m sure you can explore that script on your own. Go ahead, explore it. I&#8217;ll wait.<\/p>\n<p>Next, we need to make sure that the script is executable.<\/p>\n<pre><code class=\"bash\">chmod +x get-kubeconfig.sh\n<\/code><\/pre>\n<p>The way we&#8217;ll execute the script differs from one vendor to another. All require the name of the cluster. However, EKS needs to know the region, AKS needs the resource group, and GKE both the region and the project ID. So, the arguments of the script are different depending on which Kubernetes platform you are using.<\/p>\n<p><em>Please execute the command that follows if you are using <strong>EKS<\/strong>.<\/em><\/p>\n<pre><code class=\"bash\">.\/get-kubeconfig.sh \\\n    $(terraform output cluster_name) \\\n    $(terraform output region)\n<\/code><\/pre>\n<p><em>Please execute the command that follows if you are using <strong>AKS<\/strong>.<\/em><\/p>\n<pre><code class=\"bash\">.\/get-kubeconfig.sh \\\n    $(terraform output cluster_name) \\\n    $(terraform output resource_group)\n<\/code><\/pre>\n<p><em>Please execute the commands that follows if you are using <strong>GKE<\/strong>.<\/em><\/p>\n<pre><code class=\"bash\">.\/get-kubeconfig.sh \\\n    $(terraform output cluster_name) \\\n    $(terraform output region) \\\n    $(terraform output project_id)\n\nexport GOOGLE_APPLICATION_CREDENTIALS=$PWD\/account.json\n<\/code><\/pre>\n<p>The Kube config was created. The only thing missing for us to access the cluster is to define the environment variable <code>KUBECONFIG<\/code> so that <code>kubectl<\/code> knows where to find the connection info.<\/p>\n<pre><code class=\"bash\">export KUBECONFIG=kubeconfig.yaml\n<\/code><\/pre>\n<p>To be on the safe side, let&#8217;s output the nodes to confirm that the cluster was indeed created and that we can access it.<\/p>\n<pre><code class=\"bash\">kubectl get nodes\n<\/code><\/pre>\n<p>The output, in my case, is as follows.<\/p>\n<pre><code>NAME                       STATUS ROLES  AGE   VERSION\nip-10-0-0-250.ec2.internal Ready  &lt;none&gt; 9m47s v1.17.9-eks-4c6976\nip-10-0-1-66.ec2.internal  Ready  &lt;none&gt; 10m   v1.17.9-eks-4c6976\nip-10-0-2-235.ec2.internal Ready  &lt;none&gt; 10m   v1.17.9-eks-4c6976\n<\/code><\/pre>\n<p>Now we can move into the main subject and figure out how to install Argo CD.<\/p>\n<h2>Installing And Managing Argo CD Using Continuous Delivery Pipelines<\/h2>\n<p>We already have a pipeline that creates and manages a Kubernetes cluster using a Codefresh pipeline with Terraform steps. I will not go through that pipeline since we already covered it in the <a href=\"https:\/\/codefresh.io\/continuous-deployment\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-aws-elastic-kubernetes-service-eks\/\">EKS<\/a>, <a href=\"https:\/\/codefresh.io\/kubernetes-tutorial\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/\">AKS<\/a>, and <a href=\"https:\/\/codefresh.io\/continuous-deployment\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-google-kubernetes-engine-gke\/\">GKE<\/a> articles. You must go through those first if you haven&#8217;t already since we will continue where those left.<\/p>\n<p>Apart from ensuring that the cluster is up-and-running, we also need to ensure that Argo CD is running. That might be the last Kubernetes applications we will ever install using <code>kubectl apply<\/code>, <code>helm upgrade --install<\/code>, and similar commands. Once Argo CD is operational, it will take care of all other deployments, updates, and deletions.<\/p>\n<p>We could set up Argo CD with commands executed from a terminal, but that would be silly. Given that we already have a pipeline that manages the cluster, it makes perfect sense to extend it with Argo CD setup.<\/p>\n<p>On the first look, we could just as well extend the pipeline with a simple <code>helm upgrade --install<\/code> command. But things are not that easy. We also need the Ingress controller so that Argo CD UI is accessible through a domain. There&#8217;s more, though. Installing anything in Kubernetes means that we need a valid Kube config, so we&#8217;ll need to generate it before setting up the Ingress controller and Argo CD.<\/p>\n<p>You already saw that I prepared <code>get-kubeconfig.sh<\/code> that will take care of creating Kube config, but, for it to work, we need a few values like, for example, the name of the cluster. We can get the info we need through <code>terraform output<\/code> commands.<\/p>\n<p>All in all, we need to do the following steps after ensuring that the cluster is up-and-running.<\/p>\n<ul>\n<li>Retrieve the info about the cluster<\/li>\n<li>Generate Kube config<\/li>\n<li>Make sure that the Ingress controller is up-and-running<\/li>\n<li>Make sure that Argo CD is up-and-running<\/li>\n<\/ul>\n<p>Luckily for you, I already created a pipeline that does all that. It&#8217;s in the <code>orig<\/code> directory, so let&#8217;s use it to replace the current <code>codefresh.yaml<\/code> pipeline.<\/p>\n<p><em>Some of the files are stored in the <code>orig<\/code> directory even though we need them in the root. That might sound strange, but there is a good reason behind it. I might be experimenting with that repo. The files in the root might be configured with my info. To avoid any potential issues, I stored the &#8220;golden&#8221; version of some of the files inside that directory.<\/em><\/p>\n<pre><code class=\"bash\">cp orig\/codefresh-argocd.yml \\\n    codefresh.yml\n\ncat codefresh.yml\n<\/code><\/pre>\n<p>We&#8217;ll comment only on the new parts of that pipeline. To be more precise, only on those that were added to the previous version of the pipeline.<\/p>\n<p>The output, limited to the relevant parts, is as follows.<\/p>\n<p><em>Do not get confused if the output on your screen is different from mine. I&#8217;ll show parts of my output based on EKS. If you&#8217;re using AKS or GKE, some of the steps will be different. Nevertheless, the logic and the flow of the steps is the same, and you should not have any trouble matching your output with my explanation.<\/em><\/p>\n<pre><code class=\"yaml\">version: \"1.0\"\nstages:\n  ...\n  - apps\nsteps:\n  ...\n  apply:\n    image: hashicorp\/terraform:0.13.0\n    title: Applying Terraform\n    stage: apply\n    commands:\n      - terraform apply -auto-approve \n      - export CLUSTER_NAME=$(terraform output cluster_name)\n      - export REGION=$(terraform output region)\n      - export DESTROY=$(terraform output destroy)\n      - cf_export CLUSTER_NAME REGION DESTROY\n    ...\n  apply_app:\n    image: vfarcic\/aws-helm-kubectl:2.0.47\n    title: Applying apps\n    stage: apps\n    commands:\n      - chmod +x get-kubeconfig.sh &amp;&amp; .\/get-kubeconfig.sh $CLUSTER_NAME $REGION\n      - export KUBECONFIG=kubeconfig.yaml\n      - kubectl apply --filename https:\/\/raw.githubusercontent.com\/kubernetes\/ingress-nginx\/controller-v0.35.0\/deploy\/static\/provider\/aws\/deploy.yaml\n      - kubectl wait --namespace ingress-nginx --for=condition=ready pod --selector=app.kubernetes.io\/component=controller --timeout=120s\n      - helm upgrade --install argocd argo-cd --repo https:\/\/argoproj.github.io\/argo-helm --namespace argocd --create-namespace --version 1.6.2 --values argocd-values.yaml --wait\n    when:\n      condition:\n        all:\n          notDestroy: '\"${{DESTROY}}\" == \"false\"'\n      branch:\n        only:\n          - master\n<\/code><\/pre>\n<p>To begin with, we added <code>apps<\/code> to the list of <code>stages<\/code>. That should provide a clear separation between the <code>steps<\/code> involved in managing the applications and those from the rest of the pipeline.<\/p>\n<p>Next, we extended the <code>apply<\/code> step by adding the commands that export the variables we need. We are storing <code>CLUSTER_NAME<\/code>. In the case of AKS, we are also exporting the <code>RESOURCE_GROUP<\/code>. EKS needs the <code>REGION<\/code>, while in the case of GKE, we are retrieving both the <code>PROJECT_ID<\/code> and the <code>REGION<\/code>.<\/p>\n<p>If you remember from the previous article, we are using the Terraform <code>destroy<\/code> variable to let it know that it should obliterate the cluster. But, this time, we&#8217;ll need it as an environment variable as well. I&#8217;ll explain it&#8217;s purpose soon. For now, please note that we&#8217;re storing it as <code>DESTROY<\/code>.<\/p>\n<p>Shell <code>export<\/code> commands retain values only during the current session. Since each step is running in a different container, the session is different in each, so whatever we <code>export<\/code> in one is not available in the others. Given that we need those values in the next step, we are using the <code>cf_export<\/code> command, which makes environment variables persistent across sessions (containers, steps).<\/p>\n<p>The <code>apply_app<\/code> step belongs to the <code>apps<\/code> stage where the &#8220;real&#8221; action is happening, at least when Argo CD is concerned.<\/p>\n<p>We need a few tools in the <code>apply_app<\/code> step.<\/p>\n<p>To begin with, we have to have <code>kubectl<\/code> and <code>helm<\/code>, since we&#8217;ll use those to install the Ingress controller and Argo CD. On top of those, we need vendor-specific CLI to retrieve the Kube config. That would be <code>aws<\/code>, <code>az<\/code>, or <code>gcloud<\/code> CLI, depending on our vendor of choice. To simplify everything and save you a few minutes, I already created images with those tools. You can see that the <code>image<\/code> field of that step is set to <code>vfarcic\/aws-helm-kubectl<\/code>, <code>vfarcic\/az-helm-kubectl<\/code>, or <code>vfarcic\/gke-helm-kubectl<\/code>.<\/p>\n<p>In the case of AKS and GKE, the first command of that step is to log in. That is not needed for EKS since it uses environment variables for authentication, and we already have them defined in the pipeline.<\/p>\n<p>The first step, ignoring the one that logs us in, is making the <code>get-kubeconfig.sh<\/code> executable and running it. As we already saw when we run it locally, it creates the <code>kubeconfig.yaml<\/code> file. Further on, we&#8217;re defining the <code>KUBECONFIG<\/code> variable that tells <code>kubectl<\/code> to use a non-default path for its config.<\/p>\n<p>The rest of the steps should be straightforward. We are applying NGINX Ingress, waiting until it is ready, and executing <code>helm upgrade --install<\/code> to ensure that Argo CD is running and configured through values specified in <code>argocd-values.yaml<\/code>. If we ever need to change any aspect of it or upgrade it, all we&#8217;ll have to do is change the values in that file and push them to Git.<\/p>\n<p>Finally, that step contains a <code>when<\/code> conditional. It will be executed only when the <code>DESTROY<\/code> variable is set to <code>false<\/code>, and the build was triggered by a change in the <code>master<\/code> branch.<\/p>\n<p>That&#8217;s it as far as the parts of the pipeline related to the setup and maintenance of Argo CD are concerned. We are almost ready to let Codefresh take care of everything. The only thing missing is to make sure that Argo CD uses the correct address for accessing the UI. As you already saw from the <code>helm upgrade<\/code> commands used in the pipeline, the values are stored in <code>argocd-values.yaml<\/code>, so let&#8217;s copy the &#8220;golden&#8221; version of it and take a quick look.<\/p>\n<pre><code class=\"bash\">cp orig\/argocd-values.yaml \\\n    argocd-values.yaml\n\ncat argocd-values.yaml\n<\/code><\/pre>\n<p>The output is as follows.<\/p>\n<pre><code class=\"yaml\">server:\n  ingress:\n    enabled: true\n    hosts:\n    - acme.com\n  extraArgs:\n    insecure: true\ninstallCRDs: false\n<\/code><\/pre>\n<p>Those values should be self-explanatory. We&#8217;re enabling <code>ingress<\/code>, allowing <code>insecure<\/code> communication since we do not have TLS for our examples, and disabling the installation of CRD.<\/p>\n<p><em>Helm 3 removed the <code>install-crds<\/code> hook, so CRDs need to be installed as if they are &#8220;normal&#8221; Kubernetes resources. Think of <code>installCRDs<\/code> set to <code>false<\/code> as a workaround.<\/em><\/p>\n<p>The problematic part with those values is the host set to <code>acme.com<\/code>. I could not know in advance whether you have a &#8220;real&#8221; domain with DNS entries pointing to the external load balancer sitting in front of your cluster. So I had to hard-code a value which we are about to change to a <a href=\"http:\/\/xip.io\/\">xip.io<\/a> domain.<\/p>\n<p><em>We&#8217;ll use <a href=\"http:\/\/xip.io\/\">xip.io<\/a> since I could not assume that you have a &#8220;real&#8221; domain that you can use for the exercises or, if you do, that you configured its DNS to point to the cluster.<\/em><\/p>\n<p>To generate a <a href=\"http:\/\/xip.io\/\">xip.io<\/a> domain, we need to retrieve the IP of the external load balancer created during the installation of the Ingress controller. Unfortunately, the way how to do that differs from one provider to another. So, we&#8217;ll need to split the commands into those for GKE and AKS on the one hand, and EKS on the other.<\/p>\n<p><em>Please execute the command that follows if you are using <strong>GKE<\/strong> or <strong>AKS<\/strong>.<\/em><\/p>\n<pre><code class=\"bash\">export INGRESS_HOST=$(kubectl \\\n    --namespace ingress-nginx \\\n    get svc ingress-nginx-controller \\\n    --output jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n<\/code><\/pre>\n<p><em>Please execute the commands that follows if you are using <strong>EKS<\/strong>.<\/em><\/p>\n<pre><code class=\"bash\">export INGRESS_HOSTNAME=$(kubectl \\\n    --namespace ingress-nginx \\\n    get svc ingress-nginx-controller \\\n    --output jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\")\n\nexport INGRESS_HOST=$(\\\n    dig +short $INGRESS_HOSTNAME)\n<\/code><\/pre>\n<p>Now you should have the IP of the external load balancer stored in the environment variable <code>INGRESS_HOST<\/code>. Let&#8217;s confirm that.<\/p>\n<pre><code class=\"bash\">echo $INGRESS_HOST\n<\/code><\/pre>\n<p>The output, in my case, is as follows.<\/p>\n<pre><code>52.71.238.18\n<\/code><\/pre>\n<p><em>If you are using <strong>AWS<\/strong> and the output contains more than one IP, wait for a while longer, and repeat the <code>export<\/code> commands. If the output continues having more than one IP, choose one of them and execute <code>export INGRESS_HOST=[...]<\/code> with <code>[...]<\/code> being the selected IP.<\/em><\/p>\n<p>Now that we have the IP, let&#8217;s define the address through which we want to access Argo CD UI.<\/p>\n<pre><code class=\"bash\">export ARGO_ADDR=argocd.$INGRESS_HOST.xip.io\n<\/code><\/pre>\n<p>All that&#8217;s left is to replace the hard-coded value <code>acme.com<\/code> with the newly generated address and push the changes to the repository. Codefresh should pick it up and run a pipeline build that will execute the steps that manage the cluster, the Ingress controller, and Argo CD.<\/p>\n<pre><code class=\"bash\">cat argocd-values.yaml \\\n    | sed -e \"s@acme.com@$ARGO_ADDR@g\" \\\n    | tee argocd-values.yaml\n\ngit add .\n\ngit commit -m \"Adding Argo CD\"\n\ngit push\n<\/code><\/pre>\n<p>If you are already using Codefresh, your reaction might be to go to its UI to see the status of the newly executed build. We will not do that. Instead, we&#8217;ll use <code>codefresh<\/code> CLI to retrieve and follow the logs of the build. But, to do that, we need to find out what its ID is. We can do that by retrieving all the builds associated with the <em>cf-terraform-&#42;<\/em> pipeline.<\/p>\n<pre><code class=\"bash\">codefresh get builds \\\n    --pipeline-name cf-terraform-$K8S_PLATFORM\n<\/code><\/pre>\n<p>Please copy the ID of the newest build located at the top of the output, and paste it instead of <code>[...]<\/code> in the command that follows.<\/p>\n<pre><code class=\"bash\">export BUILD_ID=[...]\n<\/code><\/pre>\n<p>Now we can retrieve and follow the logs.<\/p>\n<pre><code class=\"bash\">codefresh logs $BUILD_ID -f\n<\/code><\/pre>\n<p>Once the build is finished, the last lines of the logs output should show the typical Helm output, including the <code>NOTES<\/code> with the next steps. We&#8217;ll ignore them for now.<\/p>\n<p>Let&#8217;s have a quick look at whether all the components of Argo CD are indeed running now.<\/p>\n<pre><code class=\"bash\">kubectl --namespace argocd get pods\n<\/code><\/pre>\n<p>The output is as follows.<\/p>\n<pre><code>NAME                              READY STATUS  RESTARTS AGE\nargocd-application-controller-... 1\/1   Running 0        15m\nargocd-dex-server-...             1\/1   Running 0        15m\nargocd-redis-...                  1\/1   Running 0        15m\nargocd-repo-server-...            1\/1   Running 0        15m\nargocd-server-...                 1\/1   Running 0        15m\n<\/code><\/pre>\n<p>Similarly, we should confirm that Argo CD Ingress is indeed set to the correct host.<\/p>\n<pre><code class=\"bash\">kubectl --namespace argocd get ingresses\n<\/code><\/pre>\n<p>The output is as follows.<\/p>\n<pre><code>NAME          HOSTS                      ADDRESS PORTS AGE\nargocd-server argocd.52.71.238.18.xip.io ...     80    25m\n<\/code><\/pre>\n<p>There&#8217;s one last step that we might need to do before we conclude that everything is working as expected.<\/p>\n<p>When installed for the first time, Argo CD uses a password that happens to be the same as the name of the Pod in which it is running. Let&#8217;s retrieve it.<\/p>\n<pre><code class=\"bash\">export PASS=$(kubectl --namespace argocd \\\n    get pods \\\n    --selector app.kubernetes.io\/name=argocd-server \\\n    --output name \\\n    | cut -d'\/' -f 2)\n<\/code><\/pre>\n<p>We stored the password in the environment variable <code>PASS<\/code>, and now we can use it to authenticate the <code>argocd<\/code> CLI. Otherwise, we wouldn&#8217;t be able to use it.<\/p>\n<pre><code class=\"bash\">argocd login \\\n    --insecure \\\n    --username admin \\\n    --password $PASS \\\n    --grpc-web \\\n    argocd.$INGRESS_HOST.xip.io\n<\/code><\/pre>\n<p>Let&#8217;s see what that password is.<\/p>\n<pre><code class=\"bash\">echo $PASS\n<\/code><\/pre>\n<p>The output, in my case, is as follows.<\/p>\n<pre><code>argocd-server-745949fb6d-jv4b5\n<\/code><\/pre>\n<p>You probably would not be able to remember that password. Even if you would, I do not want you to waste your brain capacity on such trivial information. So let&#8217;s change it to something that will be easier to remember.<\/p>\n<pre><code class=\"bash\">argocd account update-password\n<\/code><\/pre>\n<p>You will be asked to <code>enter current password<\/code>. Copy and paste the output of <code>echo $PASS<\/code>. Further on, you&#8217;ll be asked to enter a new password twice. Do it. If you&#8217;re uninspired, use <code>admin123<\/code> or something similar. After all, it&#8217;s a demo cluster, and there&#8217;s no need to be creative with special characters, rarely used words, and similar things that make passwords less likely to be guessed.<\/p>\n<p>All that&#8217;s left is to open Argo CD in a browser and confirm that it works.<\/p>\n<p><em>If you are a <strong>Linux<\/strong> or a <strong>WSL<\/strong> user, I will assume that you created the <code>alias<\/code> <code>open<\/code> and set it to the <code>xdg-open<\/code> command. If that&#8217;s not the case, you will find instructions on doing that in the <a href=\"#local-dev\">Setting Up A Local Development Environment<\/a> chapter. If you do not have the <code>open<\/code> command (or the alias), you should replace <code>open<\/code> with <code>echo<\/code> and copy and paste the output into your favorite browser.<\/em><\/p>\n<pre><code class=\"bash\">open http:\/\/$ARGO_ADDR\n<\/code><\/pre>\n<p>You should be presented with the sign-in screen.<\/p>\n<p><img src=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/argocd-login-1024x743.png\" alt=\"\" width=\"1024\" height=\"743\" class=\"aligncenter size-large wp-image-17679\" srcset=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/argocd-login-1024x743.png 1024w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/argocd-login-300x218.png 300w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/argocd-login-768x557.png 768w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/argocd-login-1536x1115.png 1536w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/argocd-login-2048x1486.png 2048w, https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/argocd-login-20x15.png 20w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" \/><\/p>\n<p>Feel free to explore the UI. But, before you do, be warned that you will not see much. We did not yet deploy a single application with Argo CD, so there&#8217;s not much excitement in the UI.<\/p>\n<h2>What Should We Do Next?<\/h2>\n<p>From here on, it&#8217;s up to you to &#8220;play&#8221; with the desired state of the cluster and the applications running inside. Change any of the declarative files in the repo, push the changes, wait until the pipeline converges your desires into reality, and observe the outcomes.<\/p>\n<p>Before you leave, remember to destroy everything. I do not want to be blamed for the expenses incurred from Cloud resources left floating after you&#8217;re finished.<\/p>\n<p>Fortunately, you already know how to destroy everything we created. You saw it in the <a href=\"https:\/\/codefresh.io\/continuous-deployment\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-aws-elastic-kubernetes-service-eks\/\">EKS<\/a>, <a href=\"https:\/\/codefresh.io\/kubernetes-tutorial\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-azure-kubernetes-service-aks\/\">AKS<\/a>, and <a href=\"https:\/\/codefresh.io\/continuous-deployment\/applying-gitops-continuous-delivery-cd-infrastructure-using-terraform-codefresh-google-kubernetes-engine-gke\/\">GKE<\/a> articles. All you have to do is open the <code>variables.tf<\/code> file, set the value of the <code>destroy<\/code> variable to <code>true<\/code>, and push the change to Git. Codefresh will take care of the rest. But, before you do that, let&#8217;s take another look at the pipeline.<\/p>\n<pre><code class=\"bash\">cat codefresh.yml\n<\/code><\/pre>\n<p>The output, limited to the <code>when<\/code> condition of the <code>apply_app<\/code> step, is as follows.<\/p>\n<pre><code class=\"yaml\">...\nsteps:\n  ...\n  apply_app:\n    ...\n    when:\n      condition:\n        all:\n          notDestroy: '\"${{DESTROY}}\" == \"false\"'\n      branch:\n        only:\n          - master\n<\/code><\/pre>\n<p>That step installs the apps. But, given that the cluster will be destroyed by the time the build reaches that step, we are making sure that it is not executed. To be more precise, we set the <code>when.condition.all.notDestroy<\/code> conditional to true only when the <code>DESTROY<\/code> value is set to <code>false<\/code>. In other words, that step will run only when the cluster is not set for destruction. There is no point in trying to deploy applications inside a non-existing cluster, doesn&#8217;t it?<\/p>\n<p>Now we can destroy the cluster. Go back to the Gist you used to create it in the first place, and follow the instructions from the <code>Destroy The Cluster<\/code> section.<\/p>\n<input class=\"fooboxshare_post_id\" type=\"hidden\" value=\"17678\"\/>","protected":false},"excerpt":{"rendered":"<p>We are about to install and manage Argo CD through a CD pipeline. &#8220;Why would we do that? We can just as well accomplish that through a command like kubectl apply or helm upgrade &#8211;install.&#8221; I&#8217;m glad you asked. The primary objective of Argo CD is to help us apply GitOps processes when deploying applications. &hellip; <a href=\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/\">Read more<\/a><\/p>\n","protected":false},"author":125,"featured_media":17787,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[3120,5467,1657,1538,6],"tags":[44,68,90,92,409,411,651,2243,3160,5497,5500,5502,5503,5504,5505,5506,5507,5508],"yoast_head":"<!-- This site is optimized with the Yoast SEO Premium plugin v17.9 (Yoast SEO v18.4.1) - https:\/\/yoast.com\/wordpress\/plugins\/seo\/ -->\n<title>How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS | Codefresh<\/title>\n<meta name=\"robots\" content=\"index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\" \/>\n<link rel=\"canonical\" href=\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/\" \/>\n<meta property=\"og:locale\" content=\"en_US\" \/>\n<meta property=\"og:type\" content=\"article\" \/>\n<meta property=\"og:title\" content=\"How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS\" \/>\n<meta property=\"og:description\" content=\"We are about to install and manage Argo CD through a CD pipeline. &#8220;Why would we do that? We can just as well accomplish that through a command like kubectl apply or helm upgrade --install.&#8221; I&#8217;m glad you asked. The primary objective of Argo CD is to help us apply GitOps processes when deploying applications. &hellip; Read more\" \/>\n<meta property=\"og:url\" content=\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/\" \/>\n<meta property=\"og:site_name\" content=\"Codefresh\" \/>\n<meta property=\"article:publisher\" content=\"https:\/\/www.facebook.com\/codefresh.io\" \/>\n<meta property=\"article:published_time\" content=\"2020-10-19T18:01:43+00:00\" \/>\n<meta property=\"article:modified_time\" content=\"2021-01-08T16:23:17+00:00\" \/>\n<meta property=\"og:image\" content=\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/install-codefresh-argo.png\" \/>\n\t<meta property=\"og:image:width\" content=\"1492\" \/>\n\t<meta property=\"og:image:height\" content=\"496\" \/>\n\t<meta property=\"og:image:type\" content=\"image\/png\" \/>\n<meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n<meta name=\"twitter:creator\" content=\"@codefresh\" \/>\n<meta name=\"twitter:site\" content=\"@codefresh\" \/>\n<meta name=\"twitter:label1\" content=\"Written by\" \/>\n\t<meta name=\"twitter:data1\" content=\"Contributor\" \/>\n\t<meta name=\"twitter:label2\" content=\"Est. reading time\" \/>\n\t<meta name=\"twitter:data2\" content=\"21 minutes\" \/>\n<script type=\"application\/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https:\/\/schema.org\",\"@graph\":[{\"@type\":\"Organization\",\"@id\":\"https:\/\/codefresh.io\/#organization\",\"name\":\"Codefresh\",\"url\":\"https:\/\/codefresh.io\/\",\"sameAs\":[\"https:\/\/www.facebook.com\/codefresh.io\",\"https:\/\/www.linkedin.com\/company\/codefresh\",\"https:\/\/www.youtube.com\/channel\/UC9r94SY6BqN05kXPIHsDXPg\",\"https:\/\/twitter.com\/codefresh\"],\"logo\":{\"@type\":\"ImageObject\",\"@id\":\"https:\/\/codefresh.io\/#logo\",\"inLanguage\":\"en-US\",\"url\":\"https:\/\/codefresh.io\/wp-content\/uploads\/2022\/02\/Codefresh_Logo_Vertical_LightBkgd.png\",\"contentUrl\":\"https:\/\/codefresh.io\/wp-content\/uploads\/2022\/02\/Codefresh_Logo_Vertical_LightBkgd.png\",\"width\":800,\"height\":800,\"caption\":\"Codefresh\"},\"image\":{\"@id\":\"https:\/\/codefresh.io\/#logo\"}},{\"@type\":\"WebSite\",\"@id\":\"https:\/\/codefresh.io\/#website\",\"url\":\"https:\/\/codefresh.io\/\",\"name\":\"Codefresh\",\"description\":\"\",\"publisher\":{\"@id\":\"https:\/\/codefresh.io\/#organization\"},\"potentialAction\":[{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https:\/\/codefresh.io\/?s={search_term_string}\"},\"query-input\":\"required name=search_term_string\"}],\"inLanguage\":\"en-US\"},{\"@type\":\"ImageObject\",\"@id\":\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#primaryimage\",\"inLanguage\":\"en-US\",\"url\":\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/install-codefresh-argo.png\",\"contentUrl\":\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/install-codefresh-argo.png\",\"width\":1492,\"height\":496},{\"@type\":\"WebPage\",\"@id\":\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#webpage\",\"url\":\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/\",\"name\":\"How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS | Codefresh\",\"isPartOf\":{\"@id\":\"https:\/\/codefresh.io\/#website\"},\"primaryImageOfPage\":{\"@id\":\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#primaryimage\"},\"datePublished\":\"2020-10-19T18:01:43+00:00\",\"dateModified\":\"2021-01-08T16:23:17+00:00\",\"breadcrumb\":{\"@id\":\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#breadcrumb\"},\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"ReadAction\",\"target\":[\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/\"]}]},{\"@type\":\"BreadcrumbList\",\"@id\":\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#breadcrumb\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Home\",\"item\":\"https:\/\/codefresh.io\/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS\"}]},{\"@type\":\"Article\",\"@id\":\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#article\",\"isPartOf\":{\"@id\":\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#webpage\"},\"author\":{\"@id\":\"https:\/\/codefresh.io\/#\/schema\/person\/268e5c2e4740502fae6d803783a16c75\"},\"headline\":\"How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS\",\"datePublished\":\"2020-10-19T18:01:43+00:00\",\"dateModified\":\"2021-01-08T16:23:17+00:00\",\"mainEntityOfPage\":{\"@id\":\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#webpage\"},\"wordCount\":3643,\"commentCount\":0,\"publisher\":{\"@id\":\"https:\/\/codefresh.io\/#organization\"},\"image\":{\"@id\":\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#primaryimage\"},\"thumbnailUrl\":\"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/install-codefresh-argo.png\",\"keywords\":[\"Kubernetes\",\"continuous delivery\",\"google cloud\",\"gke\",\"aws\",\"azure\",\"K8s\",\"cd\",\"aks\",\"Google Cloud Platform\",\"Google Kubernetes Engine\",\"Elastic Kubernetes Service\",\"EKS\",\"Microsoft Azure\",\"Azure Kubernetes Service\",\"Argo CD\",\"ArgoCD\",\"Argo\"],\"articleSection\":[\"Continuous Deployment\/Delivery\",\"Devops\",\"Kubernetes Tutorials\",\"DevOps Tutorials\",\"How Tos\"],\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"CommentAction\",\"name\":\"Comment\",\"target\":[\"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#respond\"]}]},{\"@type\":\"Person\",\"@id\":\"https:\/\/codefresh.io\/#\/schema\/person\/268e5c2e4740502fae6d803783a16c75\",\"name\":\"Contributor\",\"image\":{\"@type\":\"ImageObject\",\"@id\":\"https:\/\/codefresh.io\/#personlogo\",\"inLanguage\":\"en-US\",\"url\":\"https:\/\/secure.gravatar.com\/avatar\/26bd6dd87bf70f0c1a44721c8b3abbbd?s=96&d=blank&r=g\",\"contentUrl\":\"https:\/\/secure.gravatar.com\/avatar\/26bd6dd87bf70f0c1a44721c8b3abbbd?s=96&d=blank&r=g\",\"caption\":\"Contributor\"},\"url\":\"https:\/\/codefresh.io\/author\/contributor\/\"}]}<\/script>\n<!-- \/ Yoast SEO Premium plugin. -->","yoast_head_json":{"title":"How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS | Codefresh","robots":{"index":"index","follow":"follow","max-snippet":"max-snippet:-1","max-image-preview":"max-image-preview:large","max-video-preview":"max-video-preview:-1"},"canonical":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/","og_locale":"en_US","og_type":"article","og_title":"How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS","og_description":"We are about to install and manage Argo CD through a CD pipeline. &#8220;Why would we do that? We can just as well accomplish that through a command like kubectl apply or helm upgrade --install.&#8221; I&#8217;m glad you asked. The primary objective of Argo CD is to help us apply GitOps processes when deploying applications. &hellip; Read more","og_url":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/","og_site_name":"Codefresh","article_publisher":"https:\/\/www.facebook.com\/codefresh.io","article_published_time":"2020-10-19T18:01:43+00:00","article_modified_time":"2021-01-08T16:23:17+00:00","og_image":[{"width":1492,"height":496,"url":"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/install-codefresh-argo.png","type":"image\/png"}],"twitter_card":"summary_large_image","twitter_creator":"@codefresh","twitter_site":"@codefresh","twitter_misc":{"Written by":"Contributor","Est. reading time":"21 minutes"},"schema":{"@context":"https:\/\/schema.org","@graph":[{"@type":"Organization","@id":"https:\/\/codefresh.io\/#organization","name":"Codefresh","url":"https:\/\/codefresh.io\/","sameAs":["https:\/\/www.facebook.com\/codefresh.io","https:\/\/www.linkedin.com\/company\/codefresh","https:\/\/www.youtube.com\/channel\/UC9r94SY6BqN05kXPIHsDXPg","https:\/\/twitter.com\/codefresh"],"logo":{"@type":"ImageObject","@id":"https:\/\/codefresh.io\/#logo","inLanguage":"en-US","url":"https:\/\/codefresh.io\/wp-content\/uploads\/2022\/02\/Codefresh_Logo_Vertical_LightBkgd.png","contentUrl":"https:\/\/codefresh.io\/wp-content\/uploads\/2022\/02\/Codefresh_Logo_Vertical_LightBkgd.png","width":800,"height":800,"caption":"Codefresh"},"image":{"@id":"https:\/\/codefresh.io\/#logo"}},{"@type":"WebSite","@id":"https:\/\/codefresh.io\/#website","url":"https:\/\/codefresh.io\/","name":"Codefresh","description":"","publisher":{"@id":"https:\/\/codefresh.io\/#organization"},"potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https:\/\/codefresh.io\/?s={search_term_string}"},"query-input":"required name=search_term_string"}],"inLanguage":"en-US"},{"@type":"ImageObject","@id":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#primaryimage","inLanguage":"en-US","url":"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/install-codefresh-argo.png","contentUrl":"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/install-codefresh-argo.png","width":1492,"height":496},{"@type":"WebPage","@id":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#webpage","url":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/","name":"How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS | Codefresh","isPartOf":{"@id":"https:\/\/codefresh.io\/#website"},"primaryImageOfPage":{"@id":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#primaryimage"},"datePublished":"2020-10-19T18:01:43+00:00","dateModified":"2021-01-08T16:23:17+00:00","breadcrumb":{"@id":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#breadcrumb"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/"]}]},{"@type":"BreadcrumbList","@id":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/codefresh.io\/"},{"@type":"ListItem","position":2,"name":"How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS"}]},{"@type":"Article","@id":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#article","isPartOf":{"@id":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#webpage"},"author":{"@id":"https:\/\/codefresh.io\/#\/schema\/person\/268e5c2e4740502fae6d803783a16c75"},"headline":"How to Install and Manage Argo CD through Continuous Delivery Pipelines Using Codefresh with GKE\/AWS\/EKS\/AKS","datePublished":"2020-10-19T18:01:43+00:00","dateModified":"2021-01-08T16:23:17+00:00","mainEntityOfPage":{"@id":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#webpage"},"wordCount":3643,"commentCount":0,"publisher":{"@id":"https:\/\/codefresh.io\/#organization"},"image":{"@id":"https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#primaryimage"},"thumbnailUrl":"https:\/\/codefresh.io\/wp-content\/uploads\/2020\/10\/install-codefresh-argo.png","keywords":["Kubernetes","continuous delivery","google cloud","gke","aws","azure","K8s","cd","aks","Google Cloud Platform","Google Kubernetes Engine","Elastic Kubernetes Service","EKS","Microsoft Azure","Azure Kubernetes Service","Argo CD","ArgoCD","Argo"],"articleSection":["Continuous Deployment\/Delivery","Devops","Kubernetes Tutorials","DevOps Tutorials","How Tos"],"inLanguage":"en-US","potentialAction":[{"@type":"CommentAction","name":"Comment","target":["https:\/\/codefresh.io\/continuous-deployment\/installing-managing-argo-cd-continuous-delivery-pipelines-using-codefresh-google-kubernetes-engine-gke-aws-elastic-kubernetes-service-eks-azure-kubernetes-service-aks\/#respond"]}]},{"@type":"Person","@id":"https:\/\/codefresh.io\/#\/schema\/person\/268e5c2e4740502fae6d803783a16c75","name":"Contributor","image":{"@type":"ImageObject","@id":"https:\/\/codefresh.io\/#personlogo","inLanguage":"en-US","url":"https:\/\/secure.gravatar.com\/avatar\/26bd6dd87bf70f0c1a44721c8b3abbbd?s=96&d=blank&r=g","contentUrl":"https:\/\/secure.gravatar.com\/avatar\/26bd6dd87bf70f0c1a44721c8b3abbbd?s=96&d=blank&r=g","caption":"Contributor"},"url":"https:\/\/codefresh.io\/author\/contributor\/"}]}},"acf":[],"_links":{"self":[{"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/posts\/17678"}],"collection":[{"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/users\/125"}],"replies":[{"embeddable":true,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/comments?post=17678"}],"version-history":[{"count":0,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/posts\/17678\/revisions"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/media\/17787"}],"wp:attachment":[{"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/media?parent=17678"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/categories?post=17678"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/codefresh.io\/wp-json\/wp\/v2\/tags?post=17678"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}